{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement learning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i30MhARd7Yie"
      },
      "source": [
        "# Reinforcement learning using openAI gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXX-CzmLv3Ev"
      },
      "source": [
        "## Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcDgY_sNvyJv"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import base64, io, time, gym\n",
        "import IPython, functools\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LxdxCc9v4lP"
      },
      "source": [
        "## Initialize our environment and our agent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk3qRQ_ZwfAN",
        "outputId": "0d9b311e-119e-4292-da7d-b08bb215f437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# define the environment here.\n",
        "# It is defined using gym by OpenAI but can replace it with custom environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6-851xxxgDV",
        "outputId": "443b4b0a-aba7-49ed-9d37-ee23ee301d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# check the observation and action space of the environment\n",
        "n_observations = env.observation_space\n",
        "print(\"Environment has observation space =\", n_observations)\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment has observation space = Box(4,)\n",
            "Number of possible actions that the agent can choose from = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMlvIyYZx7vL"
      },
      "source": [
        "## Define the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tcRb_uDx67B"
      },
      "source": [
        "# Defines a feed-forward neural network\n",
        "def create_agent():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "      tf.keras.layers.Dense(n_actions)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "agent = create_agent()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7wmnyAwzYey"
      },
      "source": [
        "# choose an action from the probabilities output from model\n",
        "def choose_action(model, observation):\n",
        "  # add batch dimension to the observation\n",
        "  observation = np.expand_dims(observation, axis=0)\n",
        "\n",
        "  '''TODO: feed the observations through the model to predict the log probabilities of each possible action.'''\n",
        "  logits = model.predict(observation)\n",
        "  prob_weights = tf.nn.softmax(logits).numpy()\n",
        "  \n",
        "  '''TODO: randomly sample from the prob_weights to pick an action.\n",
        "  Hint: carefully consider the dimensionality of the input probabilities (vector) and the output action (scalar)'''\n",
        "  action = np.random.choice(n_actions, size=1, p=prob_weights[0])[0]\n",
        "\n",
        "  return action"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSoVlF5_whqV"
      },
      "source": [
        "# Define agent's memory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nprUagwvwg7t"
      },
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "  def __init__(self): \n",
        "      self.clear()\n",
        "\n",
        "  # Resets/restarts the memory buffer\n",
        "  def clear(self): \n",
        "      self.observations = []\n",
        "      self.actions = []\n",
        "      self.rewards = []\n",
        "\n",
        "  # Add observations, actions, rewards to memory\n",
        "  def add_to_memory(self, observation, action, reward): \n",
        "      self.observations.append(observation)\n",
        "      self.actions.append(action)\n",
        "      self.rewards.append(reward)\n",
        "        \n",
        "memory = Memory()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApUElhCUwgso"
      },
      "source": [
        "## Define a reward function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tt59K7AwgZt"
      },
      "source": [
        "### Reward function ###\n",
        "\n",
        "# Helper function that normalizes an np.array x\n",
        "def normalize(x):\n",
        "  x -= np.mean(x)\n",
        "  x /= np.std(x)\n",
        "  return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  R = 0\n",
        "  for t in reversed(range(0, len(rewards))):\n",
        "      # update the total discounted reward\n",
        "      R = R * gamma + rewards[t]\n",
        "      discounted_rewards[t] = R\n",
        "      \n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBJATKepwgGS"
      },
      "source": [
        "## Define the learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyA2NSqhwf0q"
      },
      "source": [
        "### Loss function ###\n",
        "\n",
        "# Arguments:\n",
        "#   logits: network's predictions for actions to take\n",
        "#   actions: the actions the agent took in an episode\n",
        "#   rewards: the rewards the agent received in an episode\n",
        "# Returns:\n",
        "#   loss\n",
        "def compute_loss(logits, actions, rewards): \n",
        "  neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
        "  loss = tf.reduce_mean(neg_logprob*rewards)\n",
        "  return loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivDNeqsXwffU"
      },
      "source": [
        "## Define training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTWfOGAG3XC8"
      },
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
        "  with tf.GradientTape() as tape:\n",
        "      logits = model(observations)\n",
        "      loss = compute_loss(logits, actions, discounted_rewards)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_hck5iT3cSw"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPYeTEtw3br6"
      },
      "source": [
        "### Cartpole training! ###\n",
        "\n",
        "# Learning rate and optimizer\n",
        "learning_rate = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "EPOCHS = 500\n",
        "\n",
        "# instantiate agent\n",
        "agent = create_agent()\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "for i_episode in range(EPOCHS):\n",
        "  # Restart the environment\n",
        "  observation = env.reset()\n",
        "  memory.clear()\n",
        "\n",
        "  while True:\n",
        "      action = choose_action(agent, observation)\n",
        "      next_observation, reward, done, info = env.step(action)\n",
        "      # add to memory\n",
        "      memory.add_to_memory(observation, action, reward)\n",
        "      if done:\n",
        "          total_reward = sum(memory.rewards)\n",
        "          train_step(agent, optimizer, \n",
        "                     observations=np.vstack(memory.observations),\n",
        "                     actions=np.array(memory.actions),\n",
        "                     discounted_rewards = discount_rewards(memory.rewards))\n",
        "          memory.clear()\n",
        "          break\n",
        "      observation = next_observation"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfEDkudH3bJ_"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Tv7_N73doA"
      },
      "source": [
        "#saved_cartpole = mdl.lab3.save_video_of_model(cartpole_model, \"CartPole-v0\")\n",
        "#mdl.lab3.play_video(saved_cartpole)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}