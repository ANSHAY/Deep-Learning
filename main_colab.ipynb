{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANSHAY/deeplearning/blob/master/main_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCeVtB27ycSQ",
        "colab_type": "text"
      },
      "source": [
        "# Poetry Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSHjcUgnycSV",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKLjP4zHycSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5Do6QhycSg",
        "colab_type": "text"
      },
      "source": [
        "## Define Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OgbrJvOycSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/romio_juliet.txt\"\n",
        "save_file_path = '/romio_juliet_predicted.txt'\n",
        "PADDING = 'pre'\n",
        "TRUNC = 'pre'\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "EMB_DIM = 128\n",
        "\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'categorical_crossentropy'\n",
        "METRICS = ['acc']\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 128\n",
        "VAL_SPLIT = 0.1\n",
        "\n",
        "SEEDER = 'That shows thee'    ## Seeder word to start prediction of poetry\n",
        "NUM_PREDICTIONS = 5000\n",
        "\n",
        "model_name = filepath[:-4] + \"_model.h5\"\n",
        "\n",
        "data = \"This is some random statement \\n being used as placeholder for the actual data that is to be \\n imported later from a file.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tsLUKOqycSo",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojrwCjKmycSq",
        "colab_type": "code",
        "outputId": "0ee9272a-c9d0-4f85-8f9d-b69cb76a19b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "with open(filepath) as f:\n",
        "    data = f.read()\n",
        "    f.close()\n",
        "data = data.replace('\\n', ' \\n<>')\n",
        "sentences = data.lower().split('<>')\n",
        "print(len(sentences))\n",
        "print(sentences[1])\n",
        "print(len(sentences[1]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4316\n",
            "prologue \n",
            "\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsocX3YaycSy",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRzGSApmycS0",
        "colab_type": "code",
        "outputId": "29deece2-af80-4483-b112-2db1b1395bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1\n",
        "print(total_words)\n",
        "print(word_index['\\n'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3765\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obq-estJycS7",
        "colab_type": "code",
        "outputId": "8deca41a-7855-41e6-c385-987ee1c4e7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## create reverse_word_index\n",
        "reverse_word_index = {}\n",
        "for word, i in word_index.items():\n",
        "    reverse_word_index[i] = word\n",
        "print(reverse_word_index[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS92FKSwycTB",
        "colab_type": "text"
      },
      "source": [
        "## Change Sentences to Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srxGiLYYycTE",
        "colab_type": "code",
        "outputId": "97d24118-71a7-4b88-8da8-ec68447ab528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "new_seq = []\n",
        "for row in sequences:\n",
        "    for i in range(2, len(row)+1):\n",
        "        new_seq.append(row[:i])\n",
        "padded_seq = pad_sequences(new_seq, padding=PADDING, truncating=TRUNC)\n",
        "print(padded_seq[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 454   4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6MhK8bmycTK",
        "colab_type": "text"
      },
      "source": [
        "## Extract trainX and trainY from sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bDsCYYrycTL",
        "colab_type": "code",
        "outputId": "458c7ffc-0580-4b17-9c42-80f876d8cd56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "trainX = padded_seq[:,:-1]\n",
        "trainY = padded_seq[:,-1]\n",
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=total_words)\n",
        "\n",
        "INP_LEN = trainX.shape[1]\n",
        "OUT_LEN = trainY.shape[1]\n",
        "\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25919, 16)\n",
            "(25919, 3765)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JbQEIe-ycTS",
        "colab_type": "text"
      },
      "source": [
        "## Define Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rYbsQCMycTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epochs, log={}):\n",
        "        if (log.get('acc')>0.90):\n",
        "            self.model.stop_training = True\n",
        "            print(\"\\n Stopped training since model reached accuracy of 90%\")\n",
        "callback = myCallback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5zVL3tHycTX",
        "colab_type": "text"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WttebE96ycTY",
        "colab_type": "code",
        "outputId": "6c2335c7-33d1-4e79-87fe-6577ac94ac9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, EMB_DIM, input_length=INP_LEN),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0716 11:35:11.017094 139690874800000 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0716 11:35:11.023859 139690874800000 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0716 11:35:11.035347 139690874800000 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0716 11:35:11.040086 139690874800000 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0716 11:35:11.042839 139690874800000 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o73E4QKKycTh",
        "colab_type": "code",
        "outputId": "e3de7a9b-bee7-420d-c50b-7e84b7f9f1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 16, 128)           481920    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3765)              244725    \n",
            "=================================================================\n",
            "Total params: 767,861\n",
            "Trainable params: 767,861\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNygXgOycTm",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XOderTm9ycTo",
        "colab_type": "code",
        "outputId": "d2316f75-1c69-48fb-eb54-88fa68788007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(trainX, trainY, validation_split=VAL_SPLIT, verbose=1, epochs = EPOCHS, batch_size=BATCH_SIZE, callbacks=[callback])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 23327 samples, validate on 2592 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0716 11:35:33.872770 139690874800000 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "23327/23327 [==============================] - 18s 774us/sample - loss: 6.5112 - acc: 0.1573 - val_loss: 6.0362 - val_acc: 0.1562\n",
            "Epoch 2/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 5.8320 - acc: 0.1606 - val_loss: 6.0091 - val_acc: 0.1562\n",
            "Epoch 3/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 5.7403 - acc: 0.1602 - val_loss: 6.0333 - val_acc: 0.1551\n",
            "Epoch 4/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 5.6778 - acc: 0.1579 - val_loss: 6.0663 - val_acc: 0.1493\n",
            "Epoch 5/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 5.6276 - acc: 0.1588 - val_loss: 6.0678 - val_acc: 0.1535\n",
            "Epoch 6/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 5.5876 - acc: 0.1607 - val_loss: 6.0805 - val_acc: 0.1570\n",
            "Epoch 7/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 5.5522 - acc: 0.1638 - val_loss: 6.0811 - val_acc: 0.1570\n",
            "Epoch 8/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 5.5163 - acc: 0.1662 - val_loss: 6.0814 - val_acc: 0.1601\n",
            "Epoch 9/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 5.4811 - acc: 0.1681 - val_loss: 6.0817 - val_acc: 0.1609\n",
            "Epoch 10/500\n",
            "23327/23327 [==============================] - 16s 697us/sample - loss: 5.4441 - acc: 0.1694 - val_loss: 6.0721 - val_acc: 0.1617\n",
            "Epoch 11/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 5.4068 - acc: 0.1704 - val_loss: 6.0725 - val_acc: 0.1593\n",
            "Epoch 12/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 5.3682 - acc: 0.1712 - val_loss: 6.0439 - val_acc: 0.1605\n",
            "Epoch 13/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 5.3188 - acc: 0.1722 - val_loss: 6.0184 - val_acc: 0.1678\n",
            "Epoch 14/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 5.2496 - acc: 0.1784 - val_loss: 5.9841 - val_acc: 0.1717\n",
            "Epoch 15/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 5.1818 - acc: 0.1863 - val_loss: 5.9595 - val_acc: 0.1806\n",
            "Epoch 16/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 5.1160 - acc: 0.1918 - val_loss: 5.9412 - val_acc: 0.1813\n",
            "Epoch 17/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 5.0507 - acc: 0.2006 - val_loss: 5.9259 - val_acc: 0.1848\n",
            "Epoch 18/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 4.9878 - acc: 0.2086 - val_loss: 5.9287 - val_acc: 0.1844\n",
            "Epoch 19/500\n",
            "23327/23327 [==============================] - 16s 699us/sample - loss: 4.9272 - acc: 0.2124 - val_loss: 5.9465 - val_acc: 0.1848\n",
            "Epoch 20/500\n",
            "23327/23327 [==============================] - 16s 697us/sample - loss: 4.8714 - acc: 0.2205 - val_loss: 5.9438 - val_acc: 0.1867\n",
            "Epoch 21/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 4.8181 - acc: 0.2276 - val_loss: 5.9435 - val_acc: 0.1840\n",
            "Epoch 22/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 4.7656 - acc: 0.2334 - val_loss: 5.9665 - val_acc: 0.1844\n",
            "Epoch 23/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 4.7167 - acc: 0.2380 - val_loss: 5.9734 - val_acc: 0.1863\n",
            "Epoch 24/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 4.6682 - acc: 0.2405 - val_loss: 5.9732 - val_acc: 0.1871\n",
            "Epoch 25/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 4.6217 - acc: 0.2422 - val_loss: 6.0018 - val_acc: 0.1856\n",
            "Epoch 26/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 4.5762 - acc: 0.2465 - val_loss: 6.0399 - val_acc: 0.1852\n",
            "Epoch 27/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 4.5314 - acc: 0.2503 - val_loss: 6.0524 - val_acc: 0.1848\n",
            "Epoch 28/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 4.4880 - acc: 0.2528 - val_loss: 6.0757 - val_acc: 0.1786\n",
            "Epoch 29/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 4.4440 - acc: 0.2555 - val_loss: 6.0918 - val_acc: 0.1825\n",
            "Epoch 30/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 4.4018 - acc: 0.2592 - val_loss: 6.1205 - val_acc: 0.1802\n",
            "Epoch 31/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 4.3594 - acc: 0.2633 - val_loss: 6.1542 - val_acc: 0.1779\n",
            "Epoch 32/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 4.3192 - acc: 0.2649 - val_loss: 6.1616 - val_acc: 0.1821\n",
            "Epoch 33/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 4.2778 - acc: 0.2672 - val_loss: 6.1980 - val_acc: 0.1794\n",
            "Epoch 34/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 4.2377 - acc: 0.2691 - val_loss: 6.2253 - val_acc: 0.1809\n",
            "Epoch 35/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 4.1981 - acc: 0.2711 - val_loss: 6.2494 - val_acc: 0.1802\n",
            "Epoch 36/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 4.1582 - acc: 0.2748 - val_loss: 6.2771 - val_acc: 0.1794\n",
            "Epoch 37/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 4.1193 - acc: 0.2763 - val_loss: 6.3074 - val_acc: 0.1806\n",
            "Epoch 38/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 4.0807 - acc: 0.2789 - val_loss: 6.3396 - val_acc: 0.1779\n",
            "Epoch 39/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 4.0416 - acc: 0.2812 - val_loss: 6.3734 - val_acc: 0.1771\n",
            "Epoch 40/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 4.0022 - acc: 0.2841 - val_loss: 6.3997 - val_acc: 0.1790\n",
            "Epoch 41/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 3.9636 - acc: 0.2870 - val_loss: 6.4351 - val_acc: 0.1790\n",
            "Epoch 42/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 3.9260 - acc: 0.2891 - val_loss: 6.4599 - val_acc: 0.1779\n",
            "Epoch 43/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 3.8885 - acc: 0.2928 - val_loss: 6.5022 - val_acc: 0.1763\n",
            "Epoch 44/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.8527 - acc: 0.2958 - val_loss: 6.5345 - val_acc: 0.1740\n",
            "Epoch 45/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 3.8175 - acc: 0.2980 - val_loss: 6.5647 - val_acc: 0.1755\n",
            "Epoch 46/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.7820 - acc: 0.2995 - val_loss: 6.5999 - val_acc: 0.1809\n",
            "Epoch 47/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.7467 - acc: 0.3029 - val_loss: 6.6316 - val_acc: 0.1798\n",
            "Epoch 48/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 3.7131 - acc: 0.3057 - val_loss: 6.6605 - val_acc: 0.1802\n",
            "Epoch 49/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 3.6811 - acc: 0.3081 - val_loss: 6.6977 - val_acc: 0.1809\n",
            "Epoch 50/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 3.6491 - acc: 0.3128 - val_loss: 6.7345 - val_acc: 0.1779\n",
            "Epoch 51/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 3.6193 - acc: 0.3166 - val_loss: 6.7676 - val_acc: 0.1767\n",
            "Epoch 52/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.5876 - acc: 0.3184 - val_loss: 6.7965 - val_acc: 0.1821\n",
            "Epoch 53/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 3.5590 - acc: 0.3213 - val_loss: 6.8165 - val_acc: 0.1829\n",
            "Epoch 54/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.5306 - acc: 0.3259 - val_loss: 6.8650 - val_acc: 0.1759\n",
            "Epoch 55/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 3.5019 - acc: 0.3288 - val_loss: 6.8959 - val_acc: 0.1771\n",
            "Epoch 56/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 3.4753 - acc: 0.3315 - val_loss: 6.9342 - val_acc: 0.1786\n",
            "Epoch 57/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 3.4464 - acc: 0.3344 - val_loss: 6.9579 - val_acc: 0.1767\n",
            "Epoch 58/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 3.4155 - acc: 0.3387 - val_loss: 6.9880 - val_acc: 0.1779\n",
            "Epoch 59/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 3.3851 - acc: 0.3431 - val_loss: 7.0266 - val_acc: 0.1802\n",
            "Epoch 60/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 3.3551 - acc: 0.3469 - val_loss: 7.0681 - val_acc: 0.1759\n",
            "Epoch 61/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 3.3248 - acc: 0.3511 - val_loss: 7.0993 - val_acc: 0.1771\n",
            "Epoch 62/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 3.2927 - acc: 0.3554 - val_loss: 7.1358 - val_acc: 0.1798\n",
            "Epoch 63/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 3.2639 - acc: 0.3592 - val_loss: 7.1670 - val_acc: 0.1802\n",
            "Epoch 64/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 3.2344 - acc: 0.3643 - val_loss: 7.2160 - val_acc: 0.1790\n",
            "Epoch 65/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 3.2074 - acc: 0.3674 - val_loss: 7.2492 - val_acc: 0.1782\n",
            "Epoch 66/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 3.1804 - acc: 0.3714 - val_loss: 7.2940 - val_acc: 0.1782\n",
            "Epoch 67/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.1538 - acc: 0.3750 - val_loss: 7.3332 - val_acc: 0.1779\n",
            "Epoch 68/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 3.1273 - acc: 0.3799 - val_loss: 7.3727 - val_acc: 0.1775\n",
            "Epoch 69/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 3.1034 - acc: 0.3814 - val_loss: 7.4165 - val_acc: 0.1771\n",
            "Epoch 70/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 3.0801 - acc: 0.3840 - val_loss: 7.4540 - val_acc: 0.1752\n",
            "Epoch 71/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 3.0564 - acc: 0.3889 - val_loss: 7.4987 - val_acc: 0.1744\n",
            "Epoch 72/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 3.0322 - acc: 0.3907 - val_loss: 7.5358 - val_acc: 0.1736\n",
            "Epoch 73/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 3.0103 - acc: 0.3942 - val_loss: 7.5704 - val_acc: 0.1748\n",
            "Epoch 74/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.9881 - acc: 0.3972 - val_loss: 7.6109 - val_acc: 0.1744\n",
            "Epoch 75/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 2.9669 - acc: 0.4005 - val_loss: 7.6459 - val_acc: 0.1709\n",
            "Epoch 76/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 2.9458 - acc: 0.4047 - val_loss: 7.6806 - val_acc: 0.1682\n",
            "Epoch 77/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.9271 - acc: 0.4065 - val_loss: 7.7396 - val_acc: 0.1736\n",
            "Epoch 78/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.9066 - acc: 0.4101 - val_loss: 7.7709 - val_acc: 0.1721\n",
            "Epoch 79/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.8867 - acc: 0.4133 - val_loss: 7.7999 - val_acc: 0.1732\n",
            "Epoch 80/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 2.8675 - acc: 0.4167 - val_loss: 7.8263 - val_acc: 0.1678\n",
            "Epoch 81/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.8494 - acc: 0.4192 - val_loss: 7.8628 - val_acc: 0.1659\n",
            "Epoch 82/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.8311 - acc: 0.4231 - val_loss: 7.9087 - val_acc: 0.1674\n",
            "Epoch 83/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.8121 - acc: 0.4252 - val_loss: 7.9536 - val_acc: 0.1632\n",
            "Epoch 84/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.7952 - acc: 0.4280 - val_loss: 7.9806 - val_acc: 0.1674\n",
            "Epoch 85/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.7777 - acc: 0.4312 - val_loss: 8.0193 - val_acc: 0.1636\n",
            "Epoch 86/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.7605 - acc: 0.4349 - val_loss: 8.0704 - val_acc: 0.1640\n",
            "Epoch 87/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.7440 - acc: 0.4373 - val_loss: 8.0804 - val_acc: 0.1632\n",
            "Epoch 88/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 2.7279 - acc: 0.4418 - val_loss: 8.1333 - val_acc: 0.1636\n",
            "Epoch 89/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.7116 - acc: 0.4433 - val_loss: 8.1473 - val_acc: 0.1644\n",
            "Epoch 90/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.6957 - acc: 0.4463 - val_loss: 8.1839 - val_acc: 0.1624\n",
            "Epoch 91/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.6815 - acc: 0.4482 - val_loss: 8.2127 - val_acc: 0.1624\n",
            "Epoch 92/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.6679 - acc: 0.4499 - val_loss: 8.2573 - val_acc: 0.1624\n",
            "Epoch 93/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.6521 - acc: 0.4531 - val_loss: 8.2771 - val_acc: 0.1632\n",
            "Epoch 94/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.6350 - acc: 0.4551 - val_loss: 8.3277 - val_acc: 0.1640\n",
            "Epoch 95/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.6227 - acc: 0.4566 - val_loss: 8.3586 - val_acc: 0.1632\n",
            "Epoch 96/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.6052 - acc: 0.4610 - val_loss: 8.3917 - val_acc: 0.1632\n",
            "Epoch 97/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.5882 - acc: 0.4627 - val_loss: 8.4099 - val_acc: 0.1617\n",
            "Epoch 98/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 2.5747 - acc: 0.4674 - val_loss: 8.4663 - val_acc: 0.1624\n",
            "Epoch 99/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.5625 - acc: 0.4657 - val_loss: 8.4805 - val_acc: 0.1651\n",
            "Epoch 100/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.5496 - acc: 0.4702 - val_loss: 8.5323 - val_acc: 0.1617\n",
            "Epoch 101/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.5351 - acc: 0.4734 - val_loss: 8.5557 - val_acc: 0.1605\n",
            "Epoch 102/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.5216 - acc: 0.4750 - val_loss: 8.5819 - val_acc: 0.1613\n",
            "Epoch 103/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.5057 - acc: 0.4768 - val_loss: 8.6182 - val_acc: 0.1628\n",
            "Epoch 104/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 2.4952 - acc: 0.4783 - val_loss: 8.6398 - val_acc: 0.1582\n",
            "Epoch 105/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.4811 - acc: 0.4827 - val_loss: 8.6637 - val_acc: 0.1617\n",
            "Epoch 106/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.4699 - acc: 0.4824 - val_loss: 8.6912 - val_acc: 0.1628\n",
            "Epoch 107/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.4575 - acc: 0.4855 - val_loss: 8.7086 - val_acc: 0.1590\n",
            "Epoch 108/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.4453 - acc: 0.4892 - val_loss: 8.7608 - val_acc: 0.1617\n",
            "Epoch 109/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.4346 - acc: 0.4903 - val_loss: 8.7842 - val_acc: 0.1605\n",
            "Epoch 110/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.4220 - acc: 0.4916 - val_loss: 8.8534 - val_acc: 0.1620\n",
            "Epoch 111/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.4088 - acc: 0.4944 - val_loss: 8.8681 - val_acc: 0.1597\n",
            "Epoch 112/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.3961 - acc: 0.4959 - val_loss: 8.8987 - val_acc: 0.1613\n",
            "Epoch 113/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.3842 - acc: 0.4983 - val_loss: 8.9201 - val_acc: 0.1578\n",
            "Epoch 114/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.3737 - acc: 0.4997 - val_loss: 8.9332 - val_acc: 0.1597\n",
            "Epoch 115/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.3620 - acc: 0.5039 - val_loss: 8.9791 - val_acc: 0.1617\n",
            "Epoch 116/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.3510 - acc: 0.5064 - val_loss: 9.0032 - val_acc: 0.1597\n",
            "Epoch 117/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 2.3392 - acc: 0.5090 - val_loss: 9.0265 - val_acc: 0.1574\n",
            "Epoch 118/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.3278 - acc: 0.5089 - val_loss: 9.0736 - val_acc: 0.1605\n",
            "Epoch 119/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.3211 - acc: 0.5113 - val_loss: 9.0938 - val_acc: 0.1574\n",
            "Epoch 120/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.3087 - acc: 0.5118 - val_loss: 9.1215 - val_acc: 0.1586\n",
            "Epoch 121/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.2989 - acc: 0.5151 - val_loss: 9.1370 - val_acc: 0.1632\n",
            "Epoch 122/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.2895 - acc: 0.5174 - val_loss: 9.1677 - val_acc: 0.1605\n",
            "Epoch 123/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.2799 - acc: 0.5187 - val_loss: 9.1704 - val_acc: 0.1586\n",
            "Epoch 124/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.2679 - acc: 0.5219 - val_loss: 9.2046 - val_acc: 0.1620\n",
            "Epoch 125/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.2569 - acc: 0.5222 - val_loss: 9.2623 - val_acc: 0.1593\n",
            "Epoch 126/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.2486 - acc: 0.5258 - val_loss: 9.2886 - val_acc: 0.1586\n",
            "Epoch 127/500\n",
            "23327/23327 [==============================] - 16s 696us/sample - loss: 2.2394 - acc: 0.5260 - val_loss: 9.2957 - val_acc: 0.1597\n",
            "Epoch 128/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.2287 - acc: 0.5280 - val_loss: 9.3205 - val_acc: 0.1566\n",
            "Epoch 129/500\n",
            "23327/23327 [==============================] - 16s 699us/sample - loss: 2.2183 - acc: 0.5291 - val_loss: 9.3618 - val_acc: 0.1582\n",
            "Epoch 130/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.2076 - acc: 0.5299 - val_loss: 9.3764 - val_acc: 0.1582\n",
            "Epoch 131/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.1997 - acc: 0.5340 - val_loss: 9.4249 - val_acc: 0.1570\n",
            "Epoch 132/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 2.1890 - acc: 0.5364 - val_loss: 9.4325 - val_acc: 0.1601\n",
            "Epoch 133/500\n",
            "23327/23327 [==============================] - 16s 698us/sample - loss: 2.1809 - acc: 0.5372 - val_loss: 9.4873 - val_acc: 0.1601\n",
            "Epoch 134/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.1706 - acc: 0.5388 - val_loss: 9.4959 - val_acc: 0.1582\n",
            "Epoch 135/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.1631 - acc: 0.5400 - val_loss: 9.5080 - val_acc: 0.1605\n",
            "Epoch 136/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.1559 - acc: 0.5422 - val_loss: 9.5671 - val_acc: 0.1597\n",
            "Epoch 137/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 2.1425 - acc: 0.5456 - val_loss: 9.5541 - val_acc: 0.1582\n",
            "Epoch 138/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 2.1354 - acc: 0.5473 - val_loss: 9.6281 - val_acc: 0.1593\n",
            "Epoch 139/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 2.1239 - acc: 0.5493 - val_loss: 9.6525 - val_acc: 0.1617\n",
            "Epoch 140/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 2.1145 - acc: 0.5481 - val_loss: 9.6549 - val_acc: 0.1632\n",
            "Epoch 141/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.1046 - acc: 0.5514 - val_loss: 9.6908 - val_acc: 0.1590\n",
            "Epoch 142/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 2.0947 - acc: 0.5556 - val_loss: 9.7104 - val_acc: 0.1582\n",
            "Epoch 143/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 2.0927 - acc: 0.5566 - val_loss: 9.7349 - val_acc: 0.1566\n",
            "Epoch 144/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 2.0867 - acc: 0.5552 - val_loss: 9.7555 - val_acc: 0.1570\n",
            "Epoch 145/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.0720 - acc: 0.5587 - val_loss: 9.8004 - val_acc: 0.1566\n",
            "Epoch 146/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.0636 - acc: 0.5605 - val_loss: 9.8026 - val_acc: 0.1613\n",
            "Epoch 147/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.0558 - acc: 0.5612 - val_loss: 9.8381 - val_acc: 0.1593\n",
            "Epoch 148/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.0463 - acc: 0.5625 - val_loss: 9.8491 - val_acc: 0.1562\n",
            "Epoch 149/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 2.0367 - acc: 0.5671 - val_loss: 9.8959 - val_acc: 0.1597\n",
            "Epoch 150/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 2.0260 - acc: 0.5680 - val_loss: 9.9328 - val_acc: 0.1617\n",
            "Epoch 151/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 2.0196 - acc: 0.5678 - val_loss: 9.9424 - val_acc: 0.1628\n",
            "Epoch 152/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 2.0153 - acc: 0.5706 - val_loss: 9.9441 - val_acc: 0.1613\n",
            "Epoch 153/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 2.0064 - acc: 0.5707 - val_loss: 9.9801 - val_acc: 0.1590\n",
            "Epoch 154/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 2.0019 - acc: 0.5707 - val_loss: 10.0101 - val_acc: 0.1590\n",
            "Epoch 155/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.9894 - acc: 0.5732 - val_loss: 10.0107 - val_acc: 0.1593\n",
            "Epoch 156/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.9812 - acc: 0.5756 - val_loss: 10.0510 - val_acc: 0.1555\n",
            "Epoch 157/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.9738 - acc: 0.5775 - val_loss: 10.0716 - val_acc: 0.1570\n",
            "Epoch 158/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.9657 - acc: 0.5789 - val_loss: 10.1054 - val_acc: 0.1570\n",
            "Epoch 159/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.9588 - acc: 0.5800 - val_loss: 10.1375 - val_acc: 0.1578\n",
            "Epoch 160/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 1.9507 - acc: 0.5823 - val_loss: 10.1846 - val_acc: 0.1555\n",
            "Epoch 161/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.9406 - acc: 0.5840 - val_loss: 10.1801 - val_acc: 0.1547\n",
            "Epoch 162/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.9342 - acc: 0.5844 - val_loss: 10.2065 - val_acc: 0.1532\n",
            "Epoch 163/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.9271 - acc: 0.5873 - val_loss: 10.2323 - val_acc: 0.1532\n",
            "Epoch 164/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.9226 - acc: 0.5857 - val_loss: 10.2895 - val_acc: 0.1508\n",
            "Epoch 165/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 1.9178 - acc: 0.5899 - val_loss: 10.2781 - val_acc: 0.1574\n",
            "Epoch 166/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.9061 - acc: 0.5909 - val_loss: 10.2883 - val_acc: 0.1551\n",
            "Epoch 167/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 1.9006 - acc: 0.5928 - val_loss: 10.3279 - val_acc: 0.1528\n",
            "Epoch 168/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.8911 - acc: 0.5944 - val_loss: 10.3758 - val_acc: 0.1547\n",
            "Epoch 169/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.8811 - acc: 0.5934 - val_loss: 10.3697 - val_acc: 0.1520\n",
            "Epoch 170/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.8746 - acc: 0.5971 - val_loss: 10.3909 - val_acc: 0.1551\n",
            "Epoch 171/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.8696 - acc: 0.5981 - val_loss: 10.4511 - val_acc: 0.1505\n",
            "Epoch 172/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.8614 - acc: 0.6002 - val_loss: 10.4317 - val_acc: 0.1512\n",
            "Epoch 173/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.8559 - acc: 0.6011 - val_loss: 10.4824 - val_acc: 0.1543\n",
            "Epoch 174/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.8468 - acc: 0.6024 - val_loss: 10.4912 - val_acc: 0.1520\n",
            "Epoch 175/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.8416 - acc: 0.6041 - val_loss: 10.5341 - val_acc: 0.1508\n",
            "Epoch 176/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.8532 - acc: 0.6000 - val_loss: 10.5392 - val_acc: 0.1497\n",
            "Epoch 177/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.8366 - acc: 0.6048 - val_loss: 10.5581 - val_acc: 0.1508\n",
            "Epoch 178/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.8257 - acc: 0.6070 - val_loss: 10.6027 - val_acc: 0.1516\n",
            "Epoch 179/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.8246 - acc: 0.6058 - val_loss: 10.6160 - val_acc: 0.1535\n",
            "Epoch 180/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.8204 - acc: 0.6065 - val_loss: 10.6126 - val_acc: 0.1528\n",
            "Epoch 181/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.8060 - acc: 0.6104 - val_loss: 10.6373 - val_acc: 0.1516\n",
            "Epoch 182/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.7974 - acc: 0.6120 - val_loss: 10.6632 - val_acc: 0.1532\n",
            "Epoch 183/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.7909 - acc: 0.6139 - val_loss: 10.6835 - val_acc: 0.1505\n",
            "Epoch 184/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.7866 - acc: 0.6151 - val_loss: 10.7189 - val_acc: 0.1489\n",
            "Epoch 185/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.7777 - acc: 0.6151 - val_loss: 10.7036 - val_acc: 0.1520\n",
            "Epoch 186/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.7697 - acc: 0.6189 - val_loss: 10.7467 - val_acc: 0.1489\n",
            "Epoch 187/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.7630 - acc: 0.6180 - val_loss: 10.7683 - val_acc: 0.1539\n",
            "Epoch 188/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.7581 - acc: 0.6210 - val_loss: 10.7753 - val_acc: 0.1497\n",
            "Epoch 189/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.7566 - acc: 0.6190 - val_loss: 10.8117 - val_acc: 0.1505\n",
            "Epoch 190/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.7537 - acc: 0.6201 - val_loss: 10.8458 - val_acc: 0.1478\n",
            "Epoch 191/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.7495 - acc: 0.6222 - val_loss: 10.8614 - val_acc: 0.1485\n",
            "Epoch 192/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.7393 - acc: 0.6237 - val_loss: 10.8778 - val_acc: 0.1493\n",
            "Epoch 193/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.7378 - acc: 0.6230 - val_loss: 10.9044 - val_acc: 0.1466\n",
            "Epoch 194/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.7280 - acc: 0.6266 - val_loss: 10.9247 - val_acc: 0.1516\n",
            "Epoch 195/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.7199 - acc: 0.6258 - val_loss: 10.9479 - val_acc: 0.1462\n",
            "Epoch 196/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.7140 - acc: 0.6273 - val_loss: 10.9566 - val_acc: 0.1493\n",
            "Epoch 197/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.7071 - acc: 0.6298 - val_loss: 10.9859 - val_acc: 0.1474\n",
            "Epoch 198/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.7021 - acc: 0.6314 - val_loss: 10.9959 - val_acc: 0.1458\n",
            "Epoch 199/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.7032 - acc: 0.6298 - val_loss: 11.0110 - val_acc: 0.1485\n",
            "Epoch 200/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.7025 - acc: 0.6296 - val_loss: 11.0190 - val_acc: 0.1497\n",
            "Epoch 201/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 1.6887 - acc: 0.6312 - val_loss: 11.0760 - val_acc: 0.1501\n",
            "Epoch 202/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.6796 - acc: 0.6357 - val_loss: 11.0928 - val_acc: 0.1462\n",
            "Epoch 203/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.6738 - acc: 0.6363 - val_loss: 11.0824 - val_acc: 0.1489\n",
            "Epoch 204/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.6673 - acc: 0.6365 - val_loss: 11.1083 - val_acc: 0.1447\n",
            "Epoch 205/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.6607 - acc: 0.6393 - val_loss: 11.1597 - val_acc: 0.1481\n",
            "Epoch 206/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.6586 - acc: 0.6405 - val_loss: 11.2006 - val_acc: 0.1474\n",
            "Epoch 207/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.6540 - acc: 0.6413 - val_loss: 11.2020 - val_acc: 0.1485\n",
            "Epoch 208/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.6484 - acc: 0.6422 - val_loss: 11.2047 - val_acc: 0.1447\n",
            "Epoch 209/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.6469 - acc: 0.6411 - val_loss: 11.2360 - val_acc: 0.1462\n",
            "Epoch 210/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.6393 - acc: 0.6441 - val_loss: 11.2637 - val_acc: 0.1470\n",
            "Epoch 211/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.6353 - acc: 0.6462 - val_loss: 11.2725 - val_acc: 0.1470\n",
            "Epoch 212/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.6323 - acc: 0.6447 - val_loss: 11.3004 - val_acc: 0.1485\n",
            "Epoch 213/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.6285 - acc: 0.6451 - val_loss: 11.2613 - val_acc: 0.1454\n",
            "Epoch 214/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.6266 - acc: 0.6450 - val_loss: 11.3122 - val_acc: 0.1470\n",
            "Epoch 215/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.6187 - acc: 0.6478 - val_loss: 11.3644 - val_acc: 0.1470\n",
            "Epoch 216/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.6169 - acc: 0.6465 - val_loss: 11.3742 - val_acc: 0.1443\n",
            "Epoch 217/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.6075 - acc: 0.6491 - val_loss: 11.4040 - val_acc: 0.1454\n",
            "Epoch 218/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.6039 - acc: 0.6519 - val_loss: 11.3964 - val_acc: 0.1447\n",
            "Epoch 219/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.5991 - acc: 0.6510 - val_loss: 11.4205 - val_acc: 0.1424\n",
            "Epoch 220/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.5953 - acc: 0.6517 - val_loss: 11.4759 - val_acc: 0.1458\n",
            "Epoch 221/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.5903 - acc: 0.6531 - val_loss: 11.5134 - val_acc: 0.1397\n",
            "Epoch 222/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.5889 - acc: 0.6540 - val_loss: 11.5066 - val_acc: 0.1447\n",
            "Epoch 223/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.5782 - acc: 0.6551 - val_loss: 11.4802 - val_acc: 0.1451\n",
            "Epoch 224/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.5757 - acc: 0.6551 - val_loss: 11.5291 - val_acc: 0.1420\n",
            "Epoch 225/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.5736 - acc: 0.6570 - val_loss: 11.5609 - val_acc: 0.1412\n",
            "Epoch 226/500\n",
            "23327/23327 [==============================] - 16s 680us/sample - loss: 1.5676 - acc: 0.6588 - val_loss: 11.5448 - val_acc: 0.1439\n",
            "Epoch 227/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.5620 - acc: 0.6586 - val_loss: 11.5840 - val_acc: 0.1431\n",
            "Epoch 228/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.5568 - acc: 0.6580 - val_loss: 11.5884 - val_acc: 0.1447\n",
            "Epoch 229/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.5553 - acc: 0.6598 - val_loss: 11.5954 - val_acc: 0.1439\n",
            "Epoch 230/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.5488 - acc: 0.6613 - val_loss: 11.6347 - val_acc: 0.1431\n",
            "Epoch 231/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.5461 - acc: 0.6620 - val_loss: 11.6709 - val_acc: 0.1412\n",
            "Epoch 232/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.5382 - acc: 0.6632 - val_loss: 11.6950 - val_acc: 0.1416\n",
            "Epoch 233/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.5326 - acc: 0.6655 - val_loss: 11.6814 - val_acc: 0.1424\n",
            "Epoch 234/500\n",
            "23327/23327 [==============================] - 16s 681us/sample - loss: 1.5342 - acc: 0.6659 - val_loss: 11.6882 - val_acc: 0.1408\n",
            "Epoch 235/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.5391 - acc: 0.6613 - val_loss: 11.7609 - val_acc: 0.1370\n",
            "Epoch 236/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.5345 - acc: 0.6612 - val_loss: 11.8037 - val_acc: 0.1400\n",
            "Epoch 237/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.5265 - acc: 0.6662 - val_loss: 11.7973 - val_acc: 0.1385\n",
            "Epoch 238/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.5136 - acc: 0.6681 - val_loss: 11.7961 - val_acc: 0.1389\n",
            "Epoch 239/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.5065 - acc: 0.6703 - val_loss: 11.8496 - val_acc: 0.1400\n",
            "Epoch 240/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.5041 - acc: 0.6707 - val_loss: 11.8659 - val_acc: 0.1393\n",
            "Epoch 241/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.4995 - acc: 0.6707 - val_loss: 11.8387 - val_acc: 0.1400\n",
            "Epoch 242/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4950 - acc: 0.6730 - val_loss: 11.9120 - val_acc: 0.1397\n",
            "Epoch 243/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.4917 - acc: 0.6737 - val_loss: 11.8796 - val_acc: 0.1431\n",
            "Epoch 244/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4921 - acc: 0.6737 - val_loss: 11.9025 - val_acc: 0.1435\n",
            "Epoch 245/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.4891 - acc: 0.6756 - val_loss: 11.9317 - val_acc: 0.1416\n",
            "Epoch 246/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4808 - acc: 0.6756 - val_loss: 11.9662 - val_acc: 0.1408\n",
            "Epoch 247/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.4786 - acc: 0.6759 - val_loss: 11.9777 - val_acc: 0.1373\n",
            "Epoch 248/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.4800 - acc: 0.6767 - val_loss: 12.0169 - val_acc: 0.1397\n",
            "Epoch 249/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.4726 - acc: 0.6752 - val_loss: 12.0345 - val_acc: 0.1366\n",
            "Epoch 250/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.4667 - acc: 0.6785 - val_loss: 12.0691 - val_acc: 0.1373\n",
            "Epoch 251/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.4653 - acc: 0.6792 - val_loss: 12.1004 - val_acc: 0.1385\n",
            "Epoch 252/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4593 - acc: 0.6805 - val_loss: 12.0919 - val_acc: 0.1412\n",
            "Epoch 253/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.4532 - acc: 0.6801 - val_loss: 12.1488 - val_acc: 0.1416\n",
            "Epoch 254/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.4473 - acc: 0.6828 - val_loss: 12.1611 - val_acc: 0.1412\n",
            "Epoch 255/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.4509 - acc: 0.6805 - val_loss: 12.1158 - val_acc: 0.1393\n",
            "Epoch 256/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.4505 - acc: 0.6816 - val_loss: 12.1483 - val_acc: 0.1408\n",
            "Epoch 257/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4449 - acc: 0.6831 - val_loss: 12.1601 - val_acc: 0.1385\n",
            "Epoch 258/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.4369 - acc: 0.6839 - val_loss: 12.1846 - val_acc: 0.1397\n",
            "Epoch 259/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.4347 - acc: 0.6843 - val_loss: 12.1704 - val_acc: 0.1389\n",
            "Epoch 260/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4345 - acc: 0.6843 - val_loss: 12.1945 - val_acc: 0.1420\n",
            "Epoch 261/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4314 - acc: 0.6855 - val_loss: 12.2521 - val_acc: 0.1420\n",
            "Epoch 262/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4304 - acc: 0.6850 - val_loss: 12.2392 - val_acc: 0.1389\n",
            "Epoch 263/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.4215 - acc: 0.6888 - val_loss: 12.2718 - val_acc: 0.1377\n",
            "Epoch 264/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.4123 - acc: 0.6901 - val_loss: 12.3034 - val_acc: 0.1404\n",
            "Epoch 265/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.4109 - acc: 0.6900 - val_loss: 12.3313 - val_acc: 0.1408\n",
            "Epoch 266/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.4093 - acc: 0.6895 - val_loss: 12.3263 - val_acc: 0.1377\n",
            "Epoch 267/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.4140 - acc: 0.6867 - val_loss: 12.3607 - val_acc: 0.1404\n",
            "Epoch 268/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.4118 - acc: 0.6892 - val_loss: 12.3726 - val_acc: 0.1389\n",
            "Epoch 269/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.4009 - acc: 0.6928 - val_loss: 12.3868 - val_acc: 0.1412\n",
            "Epoch 270/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3956 - acc: 0.6928 - val_loss: 12.3700 - val_acc: 0.1385\n",
            "Epoch 271/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.3983 - acc: 0.6932 - val_loss: 12.4158 - val_acc: 0.1377\n",
            "Epoch 272/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3915 - acc: 0.6924 - val_loss: 12.4407 - val_acc: 0.1439\n",
            "Epoch 273/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.3838 - acc: 0.6952 - val_loss: 12.4380 - val_acc: 0.1424\n",
            "Epoch 274/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3948 - acc: 0.6919 - val_loss: 12.4613 - val_acc: 0.1397\n",
            "Epoch 275/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3870 - acc: 0.6934 - val_loss: 12.4789 - val_acc: 0.1400\n",
            "Epoch 276/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3756 - acc: 0.6951 - val_loss: 12.5034 - val_acc: 0.1408\n",
            "Epoch 277/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.3709 - acc: 0.6992 - val_loss: 12.4919 - val_acc: 0.1420\n",
            "Epoch 278/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.3684 - acc: 0.6985 - val_loss: 12.5563 - val_acc: 0.1404\n",
            "Epoch 279/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.3620 - acc: 0.7009 - val_loss: 12.5879 - val_acc: 0.1385\n",
            "Epoch 280/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.3583 - acc: 0.7011 - val_loss: 12.5229 - val_acc: 0.1443\n",
            "Epoch 281/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.3751 - acc: 0.6960 - val_loss: 12.6180 - val_acc: 0.1393\n",
            "Epoch 282/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.3666 - acc: 0.6973 - val_loss: 12.6971 - val_acc: 0.1404\n",
            "Epoch 283/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.3678 - acc: 0.6978 - val_loss: 12.6521 - val_acc: 0.1393\n",
            "Epoch 284/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3630 - acc: 0.6991 - val_loss: 12.6206 - val_acc: 0.1393\n",
            "Epoch 285/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.3528 - acc: 0.6997 - val_loss: 12.6740 - val_acc: 0.1408\n",
            "Epoch 286/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.3414 - acc: 0.7046 - val_loss: 12.6833 - val_acc: 0.1393\n",
            "Epoch 287/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3431 - acc: 0.7034 - val_loss: 12.7015 - val_acc: 0.1370\n",
            "Epoch 288/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3454 - acc: 0.7033 - val_loss: 12.7147 - val_acc: 0.1385\n",
            "Epoch 289/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.3421 - acc: 0.7037 - val_loss: 12.7997 - val_acc: 0.1393\n",
            "Epoch 290/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.3371 - acc: 0.7051 - val_loss: 12.7210 - val_acc: 0.1400\n",
            "Epoch 291/500\n",
            "23327/23327 [==============================] - 16s 695us/sample - loss: 1.3327 - acc: 0.7049 - val_loss: 12.8062 - val_acc: 0.1370\n",
            "Epoch 292/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3246 - acc: 0.7070 - val_loss: 12.7529 - val_acc: 0.1385\n",
            "Epoch 293/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.3277 - acc: 0.7051 - val_loss: 12.8129 - val_acc: 0.1420\n",
            "Epoch 294/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.3233 - acc: 0.7073 - val_loss: 12.8198 - val_acc: 0.1389\n",
            "Epoch 295/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.3212 - acc: 0.7081 - val_loss: 12.8437 - val_acc: 0.1385\n",
            "Epoch 296/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.3181 - acc: 0.7091 - val_loss: 12.8780 - val_acc: 0.1377\n",
            "Epoch 297/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.3186 - acc: 0.7072 - val_loss: 12.8713 - val_acc: 0.1397\n",
            "Epoch 298/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.3178 - acc: 0.7075 - val_loss: 12.8793 - val_acc: 0.1385\n",
            "Epoch 299/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.3154 - acc: 0.7078 - val_loss: 12.9034 - val_acc: 0.1381\n",
            "Epoch 300/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.3125 - acc: 0.7077 - val_loss: 12.9265 - val_acc: 0.1397\n",
            "Epoch 301/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3070 - acc: 0.7094 - val_loss: 12.9227 - val_acc: 0.1397\n",
            "Epoch 302/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.3026 - acc: 0.7121 - val_loss: 12.9583 - val_acc: 0.1389\n",
            "Epoch 303/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3037 - acc: 0.7105 - val_loss: 12.9610 - val_acc: 0.1393\n",
            "Epoch 304/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3029 - acc: 0.7088 - val_loss: 13.0281 - val_acc: 0.1404\n",
            "Epoch 305/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.2956 - acc: 0.7118 - val_loss: 13.0534 - val_acc: 0.1397\n",
            "Epoch 306/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.2932 - acc: 0.7106 - val_loss: 13.0460 - val_acc: 0.1389\n",
            "Epoch 307/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.2943 - acc: 0.7139 - val_loss: 13.0431 - val_acc: 0.1420\n",
            "Epoch 308/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.2891 - acc: 0.7120 - val_loss: 13.0768 - val_acc: 0.1377\n",
            "Epoch 309/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2889 - acc: 0.7134 - val_loss: 13.0673 - val_acc: 0.1400\n",
            "Epoch 310/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.2822 - acc: 0.7151 - val_loss: 13.0922 - val_acc: 0.1389\n",
            "Epoch 311/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.2827 - acc: 0.7149 - val_loss: 13.0927 - val_acc: 0.1404\n",
            "Epoch 312/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2757 - acc: 0.7167 - val_loss: 13.1507 - val_acc: 0.1412\n",
            "Epoch 313/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.2804 - acc: 0.7168 - val_loss: 13.0888 - val_acc: 0.1393\n",
            "Epoch 314/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.3024 - acc: 0.7074 - val_loss: 13.1803 - val_acc: 0.1424\n",
            "Epoch 315/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2861 - acc: 0.7120 - val_loss: 13.1402 - val_acc: 0.1439\n",
            "Epoch 316/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.3126 - acc: 0.7053 - val_loss: 13.1167 - val_acc: 0.1424\n",
            "Epoch 317/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2817 - acc: 0.7145 - val_loss: 13.1570 - val_acc: 0.1447\n",
            "Epoch 318/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2685 - acc: 0.7169 - val_loss: 13.1822 - val_acc: 0.1431\n",
            "Epoch 319/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.2582 - acc: 0.7188 - val_loss: 13.2346 - val_acc: 0.1412\n",
            "Epoch 320/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.2516 - acc: 0.7219 - val_loss: 13.2751 - val_acc: 0.1416\n",
            "Epoch 321/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2449 - acc: 0.7237 - val_loss: 13.2446 - val_acc: 0.1400\n",
            "Epoch 322/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2393 - acc: 0.7254 - val_loss: 13.2507 - val_acc: 0.1408\n",
            "Epoch 323/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.2367 - acc: 0.7244 - val_loss: 13.3027 - val_acc: 0.1400\n",
            "Epoch 324/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.2413 - acc: 0.7248 - val_loss: 13.3260 - val_acc: 0.1400\n",
            "Epoch 325/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.2380 - acc: 0.7253 - val_loss: 13.3408 - val_acc: 0.1408\n",
            "Epoch 326/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.2352 - acc: 0.7260 - val_loss: 13.3352 - val_acc: 0.1431\n",
            "Epoch 327/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.2365 - acc: 0.7248 - val_loss: 13.3358 - val_acc: 0.1424\n",
            "Epoch 328/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.2367 - acc: 0.7243 - val_loss: 13.3706 - val_acc: 0.1424\n",
            "Epoch 329/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.2355 - acc: 0.7243 - val_loss: 13.3806 - val_acc: 0.1427\n",
            "Epoch 330/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.2361 - acc: 0.7235 - val_loss: 13.3845 - val_acc: 0.1424\n",
            "Epoch 331/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2429 - acc: 0.7214 - val_loss: 13.3692 - val_acc: 0.1447\n",
            "Epoch 332/500\n",
            "23327/23327 [==============================] - 16s 694us/sample - loss: 1.2361 - acc: 0.7248 - val_loss: 13.3847 - val_acc: 0.1393\n",
            "Epoch 333/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.2388 - acc: 0.7231 - val_loss: 13.4080 - val_acc: 0.1366\n",
            "Epoch 334/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.2411 - acc: 0.7208 - val_loss: 13.4898 - val_acc: 0.1377\n",
            "Epoch 335/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.2364 - acc: 0.7239 - val_loss: 13.4714 - val_acc: 0.1416\n",
            "Epoch 336/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2215 - acc: 0.7269 - val_loss: 13.5077 - val_acc: 0.1393\n",
            "Epoch 337/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.2180 - acc: 0.7270 - val_loss: 13.5280 - val_acc: 0.1397\n",
            "Epoch 338/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.2162 - acc: 0.7274 - val_loss: 13.5429 - val_acc: 0.1439\n",
            "Epoch 339/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.2108 - acc: 0.7301 - val_loss: 13.5471 - val_acc: 0.1412\n",
            "Epoch 340/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.2069 - acc: 0.7301 - val_loss: 13.5407 - val_acc: 0.1389\n",
            "Epoch 341/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.2151 - acc: 0.7282 - val_loss: 13.5366 - val_acc: 0.1420\n",
            "Epoch 342/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.2233 - acc: 0.7254 - val_loss: 13.5443 - val_acc: 0.1362\n",
            "Epoch 343/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.2080 - acc: 0.7279 - val_loss: 13.5683 - val_acc: 0.1385\n",
            "Epoch 344/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2078 - acc: 0.7323 - val_loss: 13.5701 - val_acc: 0.1385\n",
            "Epoch 345/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.2160 - acc: 0.7276 - val_loss: 13.6536 - val_acc: 0.1385\n",
            "Epoch 346/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.2064 - acc: 0.7293 - val_loss: 13.6434 - val_acc: 0.1416\n",
            "Epoch 347/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1984 - acc: 0.7328 - val_loss: 13.6897 - val_acc: 0.1416\n",
            "Epoch 348/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.1978 - acc: 0.7316 - val_loss: 13.5867 - val_acc: 0.1397\n",
            "Epoch 349/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.1975 - acc: 0.7322 - val_loss: 13.6025 - val_acc: 0.1397\n",
            "Epoch 350/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.1863 - acc: 0.7349 - val_loss: 13.6995 - val_acc: 0.1397\n",
            "Epoch 351/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1981 - acc: 0.7321 - val_loss: 13.6880 - val_acc: 0.1408\n",
            "Epoch 352/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.1938 - acc: 0.7343 - val_loss: 13.7389 - val_acc: 0.1362\n",
            "Epoch 353/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1839 - acc: 0.7349 - val_loss: 13.7122 - val_acc: 0.1393\n",
            "Epoch 354/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1802 - acc: 0.7358 - val_loss: 13.7597 - val_acc: 0.1389\n",
            "Epoch 355/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1754 - acc: 0.7373 - val_loss: 13.7723 - val_acc: 0.1373\n",
            "Epoch 356/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1744 - acc: 0.7368 - val_loss: 13.7304 - val_acc: 0.1404\n",
            "Epoch 357/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1804 - acc: 0.7354 - val_loss: 13.7484 - val_acc: 0.1373\n",
            "Epoch 358/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1856 - acc: 0.7335 - val_loss: 13.7552 - val_acc: 0.1381\n",
            "Epoch 359/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.1939 - acc: 0.7311 - val_loss: 13.7661 - val_acc: 0.1412\n",
            "Epoch 360/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.1826 - acc: 0.7344 - val_loss: 13.8226 - val_acc: 0.1400\n",
            "Epoch 361/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1705 - acc: 0.7382 - val_loss: 13.8031 - val_acc: 0.1385\n",
            "Epoch 362/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.1677 - acc: 0.7374 - val_loss: 13.8103 - val_acc: 0.1397\n",
            "Epoch 363/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.1701 - acc: 0.7382 - val_loss: 13.8538 - val_acc: 0.1354\n",
            "Epoch 364/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1710 - acc: 0.7374 - val_loss: 13.8017 - val_acc: 0.1400\n",
            "Epoch 365/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.1691 - acc: 0.7383 - val_loss: 13.8692 - val_acc: 0.1397\n",
            "Epoch 366/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1671 - acc: 0.7361 - val_loss: 13.9263 - val_acc: 0.1416\n",
            "Epoch 367/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1616 - acc: 0.7390 - val_loss: 13.9245 - val_acc: 0.1377\n",
            "Epoch 368/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.1513 - acc: 0.7417 - val_loss: 13.9329 - val_acc: 0.1397\n",
            "Epoch 369/500\n",
            "23327/23327 [==============================] - 16s 692us/sample - loss: 1.1545 - acc: 0.7406 - val_loss: 13.9610 - val_acc: 0.1397\n",
            "Epoch 370/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1499 - acc: 0.7414 - val_loss: 13.9259 - val_acc: 0.1389\n",
            "Epoch 371/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1449 - acc: 0.7420 - val_loss: 13.9081 - val_acc: 0.1377\n",
            "Epoch 372/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1419 - acc: 0.7438 - val_loss: 13.9449 - val_acc: 0.1393\n",
            "Epoch 373/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.1436 - acc: 0.7440 - val_loss: 13.9708 - val_acc: 0.1366\n",
            "Epoch 374/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1444 - acc: 0.7424 - val_loss: 14.0040 - val_acc: 0.1373\n",
            "Epoch 375/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1376 - acc: 0.7448 - val_loss: 14.0061 - val_acc: 0.1400\n",
            "Epoch 376/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.1376 - acc: 0.7443 - val_loss: 13.9914 - val_acc: 0.1420\n",
            "Epoch 377/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.1421 - acc: 0.7434 - val_loss: 14.0084 - val_acc: 0.1412\n",
            "Epoch 378/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1471 - acc: 0.7420 - val_loss: 14.0632 - val_acc: 0.1408\n",
            "Epoch 379/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1465 - acc: 0.7427 - val_loss: 14.0633 - val_acc: 0.1385\n",
            "Epoch 380/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1557 - acc: 0.7384 - val_loss: 14.0836 - val_acc: 0.1393\n",
            "Epoch 381/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1461 - acc: 0.7425 - val_loss: 14.0772 - val_acc: 0.1370\n",
            "Epoch 382/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1432 - acc: 0.7412 - val_loss: 14.1026 - val_acc: 0.1393\n",
            "Epoch 383/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1334 - acc: 0.7449 - val_loss: 14.0862 - val_acc: 0.1424\n",
            "Epoch 384/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1407 - acc: 0.7406 - val_loss: 14.0889 - val_acc: 0.1350\n",
            "Epoch 385/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1325 - acc: 0.7443 - val_loss: 14.1265 - val_acc: 0.1373\n",
            "Epoch 386/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1219 - acc: 0.7481 - val_loss: 14.1378 - val_acc: 0.1385\n",
            "Epoch 387/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.1154 - acc: 0.7476 - val_loss: 14.1307 - val_acc: 0.1397\n",
            "Epoch 388/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.1139 - acc: 0.7504 - val_loss: 14.1533 - val_acc: 0.1400\n",
            "Epoch 389/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.1178 - acc: 0.7487 - val_loss: 14.1378 - val_acc: 0.1373\n",
            "Epoch 390/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1131 - acc: 0.7487 - val_loss: 14.1857 - val_acc: 0.1408\n",
            "Epoch 391/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.1069 - acc: 0.7523 - val_loss: 14.2226 - val_acc: 0.1412\n",
            "Epoch 392/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.1043 - acc: 0.7510 - val_loss: 14.2365 - val_acc: 0.1362\n",
            "Epoch 393/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1043 - acc: 0.7508 - val_loss: 14.2273 - val_acc: 0.1385\n",
            "Epoch 394/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1081 - acc: 0.7502 - val_loss: 14.2399 - val_acc: 0.1408\n",
            "Epoch 395/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1130 - acc: 0.7490 - val_loss: 14.2394 - val_acc: 0.1381\n",
            "Epoch 396/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.1075 - acc: 0.7493 - val_loss: 14.2496 - val_acc: 0.1385\n",
            "Epoch 397/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.1103 - acc: 0.7483 - val_loss: 14.2421 - val_acc: 0.1381\n",
            "Epoch 398/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1039 - acc: 0.7499 - val_loss: 14.2906 - val_acc: 0.1377\n",
            "Epoch 399/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.1107 - acc: 0.7486 - val_loss: 14.3081 - val_acc: 0.1389\n",
            "Epoch 400/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.1036 - acc: 0.7508 - val_loss: 14.3228 - val_acc: 0.1404\n",
            "Epoch 401/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.1045 - acc: 0.7502 - val_loss: 14.2704 - val_acc: 0.1346\n",
            "Epoch 402/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.1010 - acc: 0.7508 - val_loss: 14.3077 - val_acc: 0.1350\n",
            "Epoch 403/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.0929 - acc: 0.7523 - val_loss: 14.3190 - val_acc: 0.1400\n",
            "Epoch 404/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0890 - acc: 0.7541 - val_loss: 14.3971 - val_acc: 0.1373\n",
            "Epoch 405/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0903 - acc: 0.7544 - val_loss: 14.3705 - val_acc: 0.1358\n",
            "Epoch 406/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0873 - acc: 0.7537 - val_loss: 14.4109 - val_acc: 0.1358\n",
            "Epoch 407/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.0818 - acc: 0.7557 - val_loss: 14.4095 - val_acc: 0.1400\n",
            "Epoch 408/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0793 - acc: 0.7554 - val_loss: 14.4293 - val_acc: 0.1389\n",
            "Epoch 409/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.0795 - acc: 0.7541 - val_loss: 14.4700 - val_acc: 0.1397\n",
            "Epoch 410/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0824 - acc: 0.7554 - val_loss: 14.4529 - val_acc: 0.1377\n",
            "Epoch 411/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0829 - acc: 0.7571 - val_loss: 14.4972 - val_acc: 0.1366\n",
            "Epoch 412/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.0820 - acc: 0.7556 - val_loss: 14.4611 - val_acc: 0.1381\n",
            "Epoch 413/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0837 - acc: 0.7562 - val_loss: 14.4817 - val_acc: 0.1354\n",
            "Epoch 414/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0770 - acc: 0.7556 - val_loss: 14.5385 - val_acc: 0.1381\n",
            "Epoch 415/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0851 - acc: 0.7534 - val_loss: 14.5223 - val_acc: 0.1362\n",
            "Epoch 416/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0907 - acc: 0.7533 - val_loss: 14.5292 - val_acc: 0.1362\n",
            "Epoch 417/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0770 - acc: 0.7553 - val_loss: 14.5415 - val_acc: 0.1339\n",
            "Epoch 418/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0672 - acc: 0.7584 - val_loss: 14.5583 - val_acc: 0.1354\n",
            "Epoch 419/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0832 - acc: 0.7537 - val_loss: 14.6015 - val_acc: 0.1354\n",
            "Epoch 420/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0725 - acc: 0.7564 - val_loss: 14.5973 - val_acc: 0.1366\n",
            "Epoch 421/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.0623 - acc: 0.7595 - val_loss: 14.5781 - val_acc: 0.1331\n",
            "Epoch 422/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0577 - acc: 0.7607 - val_loss: 14.6119 - val_acc: 0.1346\n",
            "Epoch 423/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0618 - acc: 0.7579 - val_loss: 14.6462 - val_acc: 0.1350\n",
            "Epoch 424/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0618 - acc: 0.7586 - val_loss: 14.6101 - val_acc: 0.1343\n",
            "Epoch 425/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0523 - acc: 0.7617 - val_loss: 14.6462 - val_acc: 0.1343\n",
            "Epoch 426/500\n",
            "23327/23327 [==============================] - 16s 693us/sample - loss: 1.0561 - acc: 0.7616 - val_loss: 14.6532 - val_acc: 0.1350\n",
            "Epoch 427/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0563 - acc: 0.7617 - val_loss: 14.6676 - val_acc: 0.1377\n",
            "Epoch 428/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0686 - acc: 0.7561 - val_loss: 14.7178 - val_acc: 0.1393\n",
            "Epoch 429/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0629 - acc: 0.7603 - val_loss: 14.7289 - val_acc: 0.1350\n",
            "Epoch 430/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0803 - acc: 0.7535 - val_loss: 14.8117 - val_acc: 0.1385\n",
            "Epoch 431/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0680 - acc: 0.7558 - val_loss: 14.7613 - val_acc: 0.1389\n",
            "Epoch 432/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0571 - acc: 0.7607 - val_loss: 14.7579 - val_acc: 0.1350\n",
            "Epoch 433/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 1.0582 - acc: 0.7608 - val_loss: 14.7395 - val_acc: 0.1339\n",
            "Epoch 434/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0563 - acc: 0.7600 - val_loss: 14.7831 - val_acc: 0.1381\n",
            "Epoch 435/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0450 - acc: 0.7647 - val_loss: 14.8249 - val_acc: 0.1358\n",
            "Epoch 436/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0324 - acc: 0.7662 - val_loss: 14.8191 - val_acc: 0.1346\n",
            "Epoch 437/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0284 - acc: 0.7673 - val_loss: 14.8253 - val_acc: 0.1389\n",
            "Epoch 438/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0301 - acc: 0.7666 - val_loss: 14.8214 - val_acc: 0.1339\n",
            "Epoch 439/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.0288 - acc: 0.7660 - val_loss: 14.8654 - val_acc: 0.1366\n",
            "Epoch 440/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0348 - acc: 0.7651 - val_loss: 14.9104 - val_acc: 0.1400\n",
            "Epoch 441/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0429 - acc: 0.7627 - val_loss: 14.8776 - val_acc: 0.1362\n",
            "Epoch 442/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0389 - acc: 0.7648 - val_loss: 14.8986 - val_acc: 0.1331\n",
            "Epoch 443/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0379 - acc: 0.7641 - val_loss: 14.9360 - val_acc: 0.1370\n",
            "Epoch 444/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.0330 - acc: 0.7657 - val_loss: 14.9689 - val_acc: 0.1346\n",
            "Epoch 445/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0242 - acc: 0.7671 - val_loss: 14.9351 - val_acc: 0.1346\n",
            "Epoch 446/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 1.0215 - acc: 0.7674 - val_loss: 14.9743 - val_acc: 0.1339\n",
            "Epoch 447/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.0232 - acc: 0.7670 - val_loss: 14.9105 - val_acc: 0.1335\n",
            "Epoch 448/500\n",
            "23327/23327 [==============================] - 16s 687us/sample - loss: 1.0197 - acc: 0.7675 - val_loss: 14.9791 - val_acc: 0.1343\n",
            "Epoch 449/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0196 - acc: 0.7672 - val_loss: 14.9912 - val_acc: 0.1373\n",
            "Epoch 450/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0190 - acc: 0.7665 - val_loss: 14.9799 - val_acc: 0.1377\n",
            "Epoch 451/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0177 - acc: 0.7700 - val_loss: 14.9395 - val_acc: 0.1370\n",
            "Epoch 452/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0270 - acc: 0.7648 - val_loss: 14.9875 - val_acc: 0.1354\n",
            "Epoch 453/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0288 - acc: 0.7642 - val_loss: 15.0544 - val_acc: 0.1354\n",
            "Epoch 454/500\n",
            "23327/23327 [==============================] - 16s 681us/sample - loss: 1.0195 - acc: 0.7665 - val_loss: 15.0223 - val_acc: 0.1350\n",
            "Epoch 455/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.0219 - acc: 0.7668 - val_loss: 15.0492 - val_acc: 0.1381\n",
            "Epoch 456/500\n",
            "23327/23327 [==============================] - 16s 690us/sample - loss: 1.0232 - acc: 0.7655 - val_loss: 15.0402 - val_acc: 0.1362\n",
            "Epoch 457/500\n",
            "23327/23327 [==============================] - 16s 689us/sample - loss: 1.0179 - acc: 0.7681 - val_loss: 15.0646 - val_acc: 0.1339\n",
            "Epoch 458/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0151 - acc: 0.7682 - val_loss: 15.0080 - val_acc: 0.1358\n",
            "Epoch 459/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 1.0167 - acc: 0.7689 - val_loss: 15.0770 - val_acc: 0.1354\n",
            "Epoch 460/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0118 - acc: 0.7688 - val_loss: 15.0847 - val_acc: 0.1362\n",
            "Epoch 461/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.0024 - acc: 0.7728 - val_loss: 15.1311 - val_acc: 0.1377\n",
            "Epoch 462/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 0.9966 - acc: 0.7731 - val_loss: 15.1143 - val_acc: 0.1389\n",
            "Epoch 463/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 0.9953 - acc: 0.7733 - val_loss: 15.1510 - val_acc: 0.1346\n",
            "Epoch 464/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9925 - acc: 0.7740 - val_loss: 15.1454 - val_acc: 0.1377\n",
            "Epoch 465/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9920 - acc: 0.7743 - val_loss: 15.1686 - val_acc: 0.1354\n",
            "Epoch 466/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 1.0073 - acc: 0.7717 - val_loss: 15.1503 - val_acc: 0.1377\n",
            "Epoch 467/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 1.0218 - acc: 0.7652 - val_loss: 15.1926 - val_acc: 0.1381\n",
            "Epoch 468/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 1.0120 - acc: 0.7674 - val_loss: 15.2064 - val_acc: 0.1393\n",
            "Epoch 469/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9983 - acc: 0.7719 - val_loss: 15.1964 - val_acc: 0.1385\n",
            "Epoch 470/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 0.9926 - acc: 0.7740 - val_loss: 15.2293 - val_acc: 0.1339\n",
            "Epoch 471/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 0.9878 - acc: 0.7766 - val_loss: 15.2300 - val_acc: 0.1377\n",
            "Epoch 472/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9873 - acc: 0.7746 - val_loss: 15.2707 - val_acc: 0.1366\n",
            "Epoch 473/500\n",
            "23327/23327 [==============================] - 16s 681us/sample - loss: 0.9866 - acc: 0.7739 - val_loss: 15.2352 - val_acc: 0.1339\n",
            "Epoch 474/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9974 - acc: 0.7716 - val_loss: 15.2632 - val_acc: 0.1312\n",
            "Epoch 475/500\n",
            "23327/23327 [==============================] - 16s 682us/sample - loss: 1.0031 - acc: 0.7710 - val_loss: 15.3626 - val_acc: 0.1358\n",
            "Epoch 476/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 0.9989 - acc: 0.7713 - val_loss: 15.2793 - val_acc: 0.1350\n",
            "Epoch 477/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9945 - acc: 0.7722 - val_loss: 15.3278 - val_acc: 0.1354\n",
            "Epoch 478/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 0.9842 - acc: 0.7754 - val_loss: 15.3036 - val_acc: 0.1346\n",
            "Epoch 479/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9805 - acc: 0.7758 - val_loss: 15.3323 - val_acc: 0.1362\n",
            "Epoch 480/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 0.9795 - acc: 0.7763 - val_loss: 15.3705 - val_acc: 0.1350\n",
            "Epoch 481/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9931 - acc: 0.7719 - val_loss: 15.3407 - val_acc: 0.1335\n",
            "Epoch 482/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 0.9879 - acc: 0.7730 - val_loss: 15.3779 - val_acc: 0.1335\n",
            "Epoch 483/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9798 - acc: 0.7766 - val_loss: 15.3703 - val_acc: 0.1323\n",
            "Epoch 484/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9982 - acc: 0.7690 - val_loss: 15.3970 - val_acc: 0.1343\n",
            "Epoch 485/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 0.9863 - acc: 0.7750 - val_loss: 15.4268 - val_acc: 0.1319\n",
            "Epoch 486/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9801 - acc: 0.7745 - val_loss: 15.3979 - val_acc: 0.1358\n",
            "Epoch 487/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 0.9769 - acc: 0.7770 - val_loss: 15.3955 - val_acc: 0.1373\n",
            "Epoch 488/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9684 - acc: 0.7788 - val_loss: 15.4802 - val_acc: 0.1343\n",
            "Epoch 489/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9626 - acc: 0.7801 - val_loss: 15.5025 - val_acc: 0.1362\n",
            "Epoch 490/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9594 - acc: 0.7811 - val_loss: 15.5035 - val_acc: 0.1358\n",
            "Epoch 491/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9630 - acc: 0.7795 - val_loss: 15.4781 - val_acc: 0.1354\n",
            "Epoch 492/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9692 - acc: 0.7801 - val_loss: 15.5078 - val_acc: 0.1343\n",
            "Epoch 493/500\n",
            "23327/23327 [==============================] - 16s 691us/sample - loss: 0.9825 - acc: 0.7727 - val_loss: 15.5010 - val_acc: 0.1331\n",
            "Epoch 494/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9900 - acc: 0.7714 - val_loss: 15.5362 - val_acc: 0.1346\n",
            "Epoch 495/500\n",
            "23327/23327 [==============================] - 16s 688us/sample - loss: 0.9782 - acc: 0.7735 - val_loss: 15.5673 - val_acc: 0.1316\n",
            "Epoch 496/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9674 - acc: 0.7770 - val_loss: 15.6060 - val_acc: 0.1323\n",
            "Epoch 497/500\n",
            "23327/23327 [==============================] - 16s 685us/sample - loss: 0.9574 - acc: 0.7804 - val_loss: 15.5683 - val_acc: 0.1358\n",
            "Epoch 498/500\n",
            "23327/23327 [==============================] - 16s 684us/sample - loss: 0.9531 - acc: 0.7804 - val_loss: 15.5767 - val_acc: 0.1335\n",
            "Epoch 499/500\n",
            "23327/23327 [==============================] - 16s 686us/sample - loss: 0.9499 - acc: 0.7832 - val_loss: 15.6143 - val_acc: 0.1331\n",
            "Epoch 500/500\n",
            "23327/23327 [==============================] - 16s 683us/sample - loss: 0.9499 - acc: 0.7813 - val_loss: 15.6433 - val_acc: 0.1335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCtmBieKycTt",
        "colab_type": "text"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulfArkaIycTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYyeJqcWycTz",
        "colab_type": "text"
      },
      "source": [
        "## Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aG380a9ycT0",
        "colab_type": "code",
        "outputId": "3dbb5ccb-81d3-4366-980b-6f255e369310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(EPOCHS)\n",
        "\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('Accuracies')\n",
        "plt.legend('Train Acc', 'Val Acc')\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.xlabel('EPOCHS')\n",
        "plt.ylabel('Losses')\n",
        "plt.legend('Train Loss', 'Val Loss')\n",
        "plt.figure()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'T' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'r' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'a' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'i' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'n' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support ' ' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'A' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'L' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/legend.py:798: UserWarning: Legend does not support 'o' instances.\n",
            "A proxy artist may be used instead.\n",
            "See: http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
            "  \"aka-proxy-artists\".format(orig_handle)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBGRfEoKIiKS7/8BpcZNHY65v0YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTaz07WaX/rkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp5I68DAycT4",
        "colab_type": "text"
      },
      "source": [
        "## Generate Poetry by iterating over the predictor from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W0pv9kmycT6",
        "colab_type": "code",
        "outputId": "d3f91093-cae7-4d07-cdf5-082c1b4b7a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sentence = SEEDER.lower()\n",
        "corpus = SEEDER.lower()\n",
        "new_line = False\n",
        "print (sentence, end=' ')\n",
        "for i in range(NUM_PREDICTIONS):\n",
        "    seq = tokenizer.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, padding=PADDING, truncating=TRUNC, maxlen=INP_LEN)\n",
        "    new_word = model.predict_classes(seq)\n",
        "    new_word = reverse_word_index[new_word[0]]\n",
        "    if new_line:\n",
        "        corpus += new_word\n",
        "        new_line = False\n",
        "    else:\n",
        "        corpus += ' ' + new_word\n",
        "    \n",
        "    if new_word=='\\n':\n",
        "        sentence = reverse_word_index[random.randint(1, total_words)]\n",
        "        #sentence = reverse_word_index[seq[0][-1]]\n",
        "        new_line = True\n",
        "        print (new_word, end='')\n",
        "    else:\n",
        "        sentence += ' ' + new_word\n",
        "        print (new_word, end=' ')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that shows thee a weak slave for the weakest goes \n",
            "till this same which dark earth there \n",
            "soul for you do not move me \n",
            "most wicked under \n",
            "by my soul i'll know the well if well \n",
            "all the kindred of the goose if dark \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "i thank nimble pinion'd doves draw love \n",
            "eyes hath well \n",
            "mercutio and benvolio \n",
            "not reply not can do but 'love' and breath \n",
            "' o then i hope thou wilt be merry \n",
            "i am fortune's fool \n",
            "for coughing in the street because he hath \n",
            "by my soul is mine at leisure earth back \n",
            "gentleman \n",
            "not so hot \n",
            "a visor for a visor what rat as years verona \n",
            "heart within tell them back them better dead why why why \n",
            "thou mayst think tybalt murdered \n",
            "\n",
            "sir i would adventure for such vile piece \n",
            "her fetch a ladder by myself \n",
            "for a man shall be in night i have \n",
            "makes the world to know there's \n",
            "night \n",
            "to clouds more clouds with his deep sighs \n",
            "page whom i said my flower \n",
            "in their own beauties or if love be bride \n",
            "in gore beauty is my convoy \n",
            "for the devout religion of him \n",
            "husband to his beard than thou hast thou another of \n",
            "sir i would not keep his windows locks alone \n",
            "\n",
            "unto my cell \n",
            "day by summer's ripening breath \n",
            "by my soul i'll come the county i will \n",
            "dun's the mouse the constable's own word \n",
            "by my soul i'll know the well if well \n",
            "for the bridegroom in this hour gives \n",
            "up the doors and would the remain will will doom \n",
            "the devil came you between us house \n",
            "from your sweet to know to me \n",
            "and warm youthful blood of \n",
            "that's not what call you to night he \n",
            "face \n",
            "not impute this \n",
            "in a minute there are many heart \n",
            "as is my greetings love to rate her eye \n",
            "i danced withal \n",
            "is a letter to the house \n",
            "any other god be late \n",
            "sir i would adventure for such a friend \n",
            "by making me despair \n",
            "by my soul i'll come the county i will \n",
            "sir i would adventure for such a veins \n",
            "five fathom early \n",
            "and in a skitless soldier's flask \n",
            "by my soul i'll know the well if well \n",
            "sounds determine of my weal or woe \n",
            "in company \n",
            "him come to thy montague \n",
            "tyrant fiend angelical \n",
            "for the county stays \n",
            "sir i would adventure for such a friend \n",
            "sorrow holy church and heaven would have \n",
            "a street \n",
            "\n",
            "by my soul good night before let's \n",
            "you be gone \n",
            "by him \n",
            "but weeps and beggary hangs upon my back \n",
            "storm of thy juliet \n",
            "in her best array bear off this dead \n",
            "in cloudy night immediately \n",
            "in thy wisdom hastes our marriage \n",
            "lies so late thou wilt undertake \n",
            "but vain fantasy \n",
            "heretics be burnt for liars \n",
            "' and then to occupy the argument no longer \n",
            "how she hath praised him girl straight come \n",
            "once hath from shrift with their merry whoreson heart chamber \n",
            "ancient death \n",
            "goodfellows paris and paris \n",
            "\n",
            "loving jealous of his liberty \n",
            "in this haste that i would tear \n",
            "the fatal an half a man \n",
            "sir i would not for the lady \n",
            "men's and why thy head hath my out wife \n",
            "look thou night said before \n",
            "in his chamber \n",
            "sir i would not keep him what is \n",
            "such other griefs in mind married \n",
            "we shall be satisfied \n",
            "\n",
            "ii \n",
            "gentleman the lark and romeo's part with cheerful \n",
            "not so green so ancient earl \n",
            "romeo hist romeo of smelling out \n",
            "in this haste \n",
            "' and then to occupy the wall wife ho \n",
            "in his foe \n",
            "a duellist a duellist a gentleman of the \n",
            "do your messages yourself \n",
            "plays and your behests again \n",
            "sir i would adventure for such vile piece \n",
            "courts thee in her best array \n",
            "monarch of the universal earth \n",
            "you up me so the make you so word \n",
            "father \n",
            "first servant \n",
            "sir i would adventure for such a friend \n",
            "will indite him to some supper \n",
            "unto my soul the sunset of him \n",
            "capulet his wife and daughters my fair niece \n",
            "you to my chamber wife \n",
            "not she is the worst sun out \n",
            "by my soul i'll come the county i will \n",
            "on thy word bring break my wedding rosemary \n",
            "\n",
            "twenty such a tortoise hung \n",
            "bills from potpan or paris \n",
            "my sail on lusty gentlemen \n",
            "marriage i'll find him own thoughts heaven or \n",
            "sir i would adventure for such a veins \n",
            "\n",
            "blood to the garish flower \n",
            "it would the thing on my thumb sound in shame \n",
            "the volume of love a encounter of \n",
            "by my soul i'll come the county i will \n",
            "she gallops o'er a misbehaved and his legs \n",
            "such one i am too swift as it art \n",
            "been a lover borrow cupid's wings \n",
            "within the kinsman of such vile submission \n",
            "that would have kill'd \n",
            "sir i would not keep from the men it \n",
            "thou love a woman \n",
            "face \n",
            "heart on some meteor why a house of \n",
            "we do exile him hence \n",
            "last night \n",
            "bethink you be a bride \n",
            "love for pricking and you beat love down \n",
            "how to speak and \n",
            "maria what a deal of brine \n",
            "but love her ripe to be put to myself \n",
            "in his needy shop a man have \n",
            "thing against sweet sweet nurse \n",
            "sir i would not keep from my conduct \n",
            "sir the more i cannot bound \n",
            "better now no wit to ne'er so faith but gone \n",
            "is the matter will thou be \n",
            "death to friar to be joyful rest \n",
            "is to tell me no wit then \n",
            "this same other which the wind \n",
            "ancient death \n",
            "home so unsatisfied \n",
            "thou hast slander'd it \n",
            "here's me into the fairies' nurse \n",
            "that the time love \n",
            "is thirty \n",
            "lick their fingers \n",
            "sir i would adventure for such a friend \n",
            "well thou hast done so wise \n",
            "villain is \n",
            "capulet and nurse \n",
            "sir i would adventure for such a friend \n",
            "that's not the night \n",
            "fingers therefore he that you shall \n",
            "in a despised spirit of \n",
            "itself turns vice being misapplied \n",
            "in their own mantle \n",
            "thou hast comforted me near he's days \n",
            "nurse and nurse \n",
            "to him and yet i have us take \n",
            "of the house of capulet's morning \n",
            "for the earth to be as much tears thou rosemary \n",
            "in this haste was ware \n",
            "' and then to occupy the wall wife ho \n",
            "young and being night \n",
            "gentleman the lark itself lips o god in heaven \n",
            "goodfellows ah that threaten'd gentleman \n",
            "his chamber light \n",
            "her best array bear her father that \n",
            "the fatal an half a man \n",
            "gentleman the lark itself lips o god in heaven \n",
            "coz i have no treason \n",
            "all the bark a man need you i \n",
            "you shall miss \n",
            "is dead and the likeness \n",
            "day by summer's ripening breath \n",
            "romeo is exiled \n",
            "what for i do you will you burden morning's \n",
            "sir i would adventure for such a veins \n",
            "in this hour i can i love \n",
            "romeo is exiled \n",
            "held here in the house to morrow my grave \n",
            "\n",
            "loving jealous of his liberty \n",
            "by my soul i'll come the county i will \n",
            "men with such sour and with again \n",
            "that i say madam go you make true days \n",
            "with thy beauty to rate her so \n",
            "body that may be love \n",
            "such a greeting villain am \n",
            "\n",
            "unto the wall or kill \n",
            "reason \n",
            "it romeo and nothing can you bite near \n",
            "thou fall in splendor of this \n",
            "romeo \n",
            "music to attending measure \n",
            "night \n",
            "by my soul i'll come the county i will \n",
            "late thou wilt stand me with my soul \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "as i know not better here my son \n",
            "books \n",
            "are you to my ghostly strength and stay \n",
            "keeps his watch in every old man's eye \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "that we both were here in heaven \n",
            "i thank nimble pinion'd doves draw love \n",
            "with such a shirt and his bloody sight \n",
            "the immortal passado the punto reverso the \n",
            "love for pricking and blows with us wife \n",
            "i will not budge for no note of him \n",
            "\n",
            "she gallops o'er a courtier's nose \n",
            "not reply not can do but 'love' and breath \n",
            "peter \n",
            "is my mother cast me not if the cause too \n",
            "and in off the day of substance in his followers \n",
            "how he is an old murderer which indeed \n",
            "romeo is exiled \n",
            "and cut him that's not so apt of slain \n",
            "' and then to occupy the argument no foot \n",
            "the infant rind of this small flower \n",
            "save the prettiest weary give me \n",
            "sir i would adventure for such a veins \n",
            "by my soul i'll come the county i will \n",
            "sir i would adventure for such a friend \n",
            "hard and be slain ne'er them measure \n",
            "fie what are you mad \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "to be sick o' both married sir let me die here \n",
            "me tell me that can i beseech 'ay ' \n",
            "you fiery letters back shadows gentleman \n",
            "is my lady thirty servant \n",
            "plays my buried days \n",
            "than she are happy mothers made \n",
            "not it makes me cords say musicians \n",
            "compare this be in this on form saw i \n",
            "and cut him that's not so apt of slain \n",
            "a ring to lose a crow me \n",
            "eyes is fire o' arm bite this do straight \n",
            "sounds good best \n",
            "you be ready do you be gone be wit near \n",
            "summer hath not such a flower \n",
            "in either and make her silver love \n",
            "\n",
            "\n",
            "sir i would adventure for such a friend \n",
            "her wife against this wedding day \n",
            "for and in a minute we \n",
            "by my soul i'll come the county i will \n",
            "gentlemen good den thou art thou leave thee \n",
            "art thou met with him for a letter \n",
            "what to what will have come and strength \n",
            "peter \n",
            "too much in these new abroach \n",
            "light is quiet or doubt \n",
            "by the charm of name of tears \n",
            "sir ah ' o then be not \n",
            "lies that ornament to attending measure \n",
            "lives that i am proof against this ancient death \n",
            "thou fall in splendor of this \n",
            "nurse and nurse \n",
            "you be patient was near be cause \n",
            "tears the world why is thou long \n",
            "confession finds but riddling shrift \n",
            "\n",
            "stir it not can you the law of her fan's the \n",
            "perforce with wilful choler meeting \n",
            "romeo of friar laurence \n",
            "sir the county paris to murder her \n",
            "for a visor for the other sends \n",
            "eyes said benvolio \n",
            "by my soul i'll come the county i will \n",
            "sir i would adventure for love \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "torches here all eyes montagues \n",
            "gentleman the lark itself lips o god in heaven \n",
            "out for this same shall need it bitter said eyes \n",
            "is not what i \n",
            "are the steerage of this fray is it \n",
            "and hero hildings and harlots thisbe a grey \n",
            "of thy house for apothecary \n",
            "fetch drier logs \n",
            "were the more of my cousin's great \n",
            "gentleman and tell him and cut him at \n",
            "night \n",
            "will you pluck your sword out of his pitcher \n",
            "comes heart for asked for and be not \n",
            "sir i would adventure for \n",
            "of vitravio signior placentio and his lovely \n",
            "that i will show you shining at mine \n",
            "for a visor for the other sends \n",
            "is the romeo and loves let's lips i warrant \n",
            "gentle night commend me \n",
            "\n",
            "cricket's afraid to stand alone \n",
            "by my extremes and your dog good right hell off \n",
            "\n",
            "how now for a ring to thursday early \n",
            "sir in that be romeo \n",
            "god more courtship lives come \n",
            "many house the loss \n",
            "lady too well of capulet's house \n",
            "god nay she comes on her bier \n",
            "art thou alack my child is dead \n",
            "you be not to a man you all \n",
            "purged a grief \n",
            "man for cracking nuts having no note of this \n",
            "is the fairies' midwife and she comes \n",
            "a plague in both must do but the \n",
            "purged \n",
            "me since well now balthasar \n",
            "sir i would adventure for such a very early sight bride \n",
            "gentlemen you love thy sallow cheeks took \n",
            "how my sword i pray it live to night \n",
            "gentlemen i warrant you merry \n",
            "twenty years and marriage but kill put off the sight \n",
            "in their hearts \n",
            "and beggary hangs upon my child be \n",
            "now my headstrong where have you been gadding \n",
            "death lain with a man news for he \n",
            "sir i would i were to me that \n",
            "but weeps and beggary hangs upon my back \n",
            "showering in one little body \n",
            "in your mother is a shame \n",
            "i am not \n",
            "thou met with him send thy man away \n",
            "of lead bright smoke cold fire \n",
            "count they are an ill love to morrow \n",
            "in their spheres till heaven bless thee count \n",
            "forth day's path and titan's fiery wheels \n",
            "in their spheres till heaven bless thee count \n",
            "sir i would adventure for such a friend \n",
            "mercutio and benvolio \n",
            "i am proof \n",
            "sir i would not keep from the men it \n",
            "god i see before the music with any wide the \n",
            "father \n",
            "and then i hope thou wilt be merry \n",
            "by my soul i'll come the county i will \n",
            "twenty years and in a minute like sweet seeming sin \n",
            "coz i said away of now \n",
            "too much minded as a rat \n",
            "\n",
            "earth to earth resign end motion here \n",
            "\n",
            "with thy head hath one in love allow \n",
            "and weeping weeping and blubbering \n",
            "a man sweet division \n",
            "in his needy shop a man have \n",
            "better sir for my brother's child \n",
            "fight tybalt falls \n",
            "home to my chamber \n",
            "in his flirt gills i am a virtuous \n",
            "many thousand times go by \n",
            "what say you simon catling \n",
            "is she \n",
            "sir \n",
            "unaccustom'd dram grave \n",
            "tears back and with me inquire you \n",
            "is the course i will give cure \n",
            "get his head is she here \n",
            "sir the county paris to murder true \n",
            "romeo and then to occupy the mansion \n",
            "\n",
            "' and then i defy you stars \n",
            "i will make a club dash out him dead \n",
            "lady too early \n",
            "in his flirt gills i am no \n",
            "heart or find delight \n",
            "rests my any thing to night \n",
            "fingers above the price of \n",
            "in his flirt gills i am a virtuous \n",
            "sir \n",
            "of the wanton blood of ours shed sin \n",
            "by their grave beseeming ornaments \n",
            "ring on his father's be well \n",
            "art thou try pathways to stay \n",
            "death to chide among these away \n",
            "be in scarlet straight at any news \n",
            "in his needy shop a thousand man lord \n",
            "home be laurence \n",
            "gentleman the lark itself lips o god in heaven \n",
            "but 'ay ' o o she not have done \n",
            "thou shalt tell her match'd and joy adieu \n",
            "music with her silver sound \n",
            "by my soul i'll know the well if well \n",
            "gentle romeo with rosaline ere both \n",
            "quarrel and paris \n",
            "for the frozen thing to my ghostly strength or sweet \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "how he hath still been tried a soul i trow \n",
            "by my soul is mine at leisure hands have goose \n",
            "sir i would adventure for such a veins \n",
            "of our house of tears himself thither \n",
            "you be romeo and found him at his veins \n",
            "discourses in his name \n",
            "and a blessed is hither shall it \n",
            "for thine to ne'er wear love and counsellor \n",
            "bills and i ordained den \n",
            "night \n",
            "you quarrel as good a nurse \n",
            "reason but she can i live banished \n",
            "livia signior valentio and his cousin \n",
            "now the prince expressly doth sworn \n",
            "in thy wisdom thou art below \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "good \n",
            "reason but she hath wedded i will carry too \n",
            "you civil brawls bred of an other maid \n",
            "romeo hist romeo of smelling out \n",
            "bandying in verona streets \n",
            "' lady capulet \n",
            "ancient soul \n",
            "i will die so i \n",
            "villain romeo \n",
            "down her ancient young if lead be men \n",
            "what murderer which way to mantua \n",
            "coz \n",
            "in a despised day is love \n",
            "being dared \n",
            "up my tongue and how my sword ho her \n",
            "shall be but a man o play \n",
            "good meat and as thou darest he \n",
            "\n",
            "three capulet come forth her face \n",
            "servant \n",
            "were some grief \n",
            "from the wanton blood up in your cheeks \n",
            "sir i would adventure for such a friend \n",
            "here's a scratch a month \n",
            "would have cull'd such \n",
            "choose but ever weep no father for before \n",
            "out to romeo only \n",
            "is the lord and wife \n",
            "in thy lips o trespass sweetly urged \n",
            "opposite to what thou justly seem'st \n",
            "is it not like a thousand times \n",
            "by my soul i'll come the county i will \n",
            "that the slip can you swear by \n",
            "gentleman the lark itself lips o god in heaven \n",
            "in his flirt gills i am no \n",
            "back again ran to hide abuse \n",
            "is she \n",
            "romeo hist romeo \n",
            "not so late good why o \n",
            "love's shadows are if it be \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "\n",
            "being dared \n",
            "their verona \n",
            "but love her ripe to be put to myself \n",
            "day a crutch why it without o'er her old \n",
            "your disposition to be gone \n",
            "it would tear the very leaves the friar \n",
            "fearful date for himself down \n",
            "thou shalt be a lovely gentleman \n",
            "gentleman the lark itself lips o god in such \n",
            "ha ha well wednesday is too soon \n",
            "shall you bite the high sweet man's dead attempt \n",
            "all my buried ancestors will \n",
            "sir i would adventure for such a friend \n",
            "is cross \n",
            "in the marriage when and where she speaks \n",
            "ancient grudge break her hap to years \n",
            "gentleman and potpan that thou dismember'd die \n",
            "sir \n",
            "thy father and refuse thy name \n",
            "having an husband pardon more on a man \n",
            "you fiery letters back shadows gentleman \n",
            "sir i would adventure for such a friend \n",
            "ancient earth hath this time \n",
            "bridal flowers serve for a buried corse \n",
            "i were sleep in a fortune nurse \n",
            "at the mangled speak \n",
            "a tender kiss meat and honour can i orchard mother \n",
            "man romeo take it him to marry along \n",
            "out on any truly were romeo \n",
            "by the vile time sir \n",
            "eye discourses i am fortune's too \n",
            "men gregory remember thy are window \n",
            "a little name and if the rest \n",
            "sir i would adventure for such a veins \n",
            "come the county can you disturb which love \n",
            "by my soul i'll come the county i will \n",
            "ancient lady farewell \n",
            "out upon this view \n",
            "many feign as they must contrary \n",
            "you civil brawls bred of an other maid \n",
            "\n",
            "coz i rather weep \n",
            "better temper'd \n",
            "thy sallow cheeks shall climb \n",
            "ancient soul \n",
            "what she loved the bridegroom eyes joy \n",
            "and weeping me tidings of lead \n",
            "the fore finger of an alderman \n",
            "in triumph and mercutio she and maid die \n",
            "were me not let her age the tears \n",
            "o musicians because the prettiest sententious wife \n",
            "no trust \n",
            "romeo's arm stabs mercutio lady montague \n",
            "ruled by my unworthiest mind misgives \n",
            "on thy word bring break my wedding rosemary \n",
            "\n",
            "cupboard look to the plate good thou save \n",
            "quarrel and paris \n",
            "man upon the high top gentleman and kiss \n",
            "sir i would not keep from the very sound \n",
            "in his needy shop a thousand man lord \n",
            "sir i would adventure for such a veins \n",
            "in his foe \n",
            "in gore beauty on our drift \n",
            "coz \n",
            "man for your rude brawls doth grace than the \n",
            "a blessed moon i take away \n",
            "gentleman the lark itself lips o god in heaven \n",
            "enough in this be in this on her \n",
            "night hath my lady farewell \n",
            "sir in this city visiting the sick \n",
            "and spurs switch and spurs is to me \n",
            "a man you are an i might touch \n",
            "attend it of a bed of \n",
            "goodfellows ah two and many brawl \n",
            "home for the matter \n",
            "gentleman so madam \n",
            "for the world was that work away now \n",
            "than paris these are news indeed \n",
            "in this fair volume is my wedding bed \n",
            "of the best but she will \n",
            "now near night \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "bills and partisans strike beat them down \n",
            "me not the serving creature's well you pray \n",
            "for the world to know there's a sea a \n",
            "tybalt the reason of these which once \n",
            "other beauties \n",
            "lady bade her new last night stands \n",
            "by the wanton blood up in my own \n",
            "' o to be absolved \n",
            "sir the county paris to prepare behold \n",
            "for a pair of star distemperature \n",
            "with corns will have a bout with you \n",
            "how to lose a tortoise mammet \n",
            "me this \n",
            "a dump we go with a man \n",
            "choose but ever weep no father for before \n",
            "in my heel \n",
            "cords vile heart \n",
            "to scratch a man to death a braggart a \n",
            "' lady capulet \n",
            "by my soul i'll come the county i will \n",
            "like an cold hid dancing prostrate term \n",
            "to be frank and give it be me as i will \n",
            "blood to the capulets therefore among my \n",
            "truly in such falsehood i'll not to him \n",
            "are you busy ho need you all \n",
            "detestable death by thee beguil'd \n",
            "words the county or 'tis married \n",
            "cupid death \n",
            "thou that exile is banished \n",
            "i \n",
            "shall i hear them take thee gone \n",
            "by my soul i'll come the county i will \n",
            "weeping weeping and therefore have thou a years \n",
            "the way to occupy himself dance \n",
            "late good mercutio the gallant away \n",
            "sir i would adventure for such \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "you be not to a man you all \n",
            "peppered i hope thou wilt not keep him \n",
            "to be absolved \n",
            "sir i would not keep from my conduct \n",
            "day o hateful me banished \n",
            "there before the night \n",
            "part a spirit in your paris' to \n",
            "i danced withal \n",
            "body that kiss and be patient looks \n",
            "that is the powerful grace \n",
            "a cold life for me how have before \n",
            "in a despised spirit ' \n",
            "me and i will not send my child \n",
            "early walking did be put there \n",
            "cords \n",
            "this same ancient feast of fire \n",
            "met the kinsman to the house \n",
            "a ring to lose a crow me \n",
            "sir i would adventure for such a veins \n",
            "for the wanton summer to me \n",
            "is coming to die \n",
            "on thy kinsman came 'zounds gregory to this \n",
            "peter \n",
            "for the heads of the maids \n",
            "that we should be thus afflicted with \n",
            "gentle romeo slain do i dwell \n",
            "were one i am sorry that the law \n",
            "\n",
            "a great ado a dream love villain gentleman \n",
            "blood to the garish flower \n",
            "sir i would adventure for love \n",
            "by my soul i'll know the well if well \n",
            "\n",
            "can you bite my thumb sir \n",
            "eye discourses loves again above \n",
            "music to attending measure \n",
            "\n",
            "old hare quoth either but romeo slain \n",
            "unto thee \n",
            "quoth my teeth \n",
            "\n",
            "' and then to occupy the argument no foot \n",
            "by her devise \n",
            "in their spheres till thou seest \n",
            "the hour of nine \n",
            "a divine a bird's stools remove the \n",
            "within the kinsman of such vile submission \n",
            "by my soul i'll come the county i will \n",
            "who thou been a tender thing in the maid \n",
            "of this fair corse and romeo \n",
            "to be sick and hoar shows here kill'd \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "in his needy shop that dream on court'sies straight \n",
            "what says nurse \n",
            "me not \n",
            "but murders pardoning those eyes take \n",
            "to stop the doors and camest thou dead \n",
            "sir i would adventure for such a friend \n",
            "once hath from shrift with their merry whoreson heart chamber \n",
            "the county go you will make short work \n",
            "sir i would not keep from the very sound \n",
            "coz i rather weep \n",
            "\n",
            "sir i would adventure for such a grief \n",
            "it for the slip can it \n",
            "peter than cousin banish'd of my maidenhead \n",
            "sir i would adventure for such a friend \n",
            "arm again to the ground took wife \n",
            "\n",
            "hast up with a courtier's nose \n",
            "on be sick for woful day \n",
            "by my soul i'll come the county i will \n",
            "soon as her lord i should forget alone \n",
            "this shall not not the serving creature's dagger on \n",
            "early \n",
            "romeo hist romeo \n",
            "in a courtier's nose \n",
            "violent hath black brow'd heaven \n",
            "death to chide among these away \n",
            "gentleman the lark itself lips o god in heaven \n",
            "fearful date for himself down \n",
            "get you sir it even a death \n",
            "thou hast comforted me marvellous much \n",
            "vows as heaven so black \n",
            "you be romeo and found him at his veins \n",
            "a piece of marchpane and as my bosom henceforth \n",
            "very like a fair good of her tomb \n",
            "all the admired beauties of verona \n",
            "devil wilt woo for the reason that spoke \n",
            "get you to church i am this is \n",
            "by the charm of flies now company \n",
            "displeased my father to laurence' cell \n",
            "sir i would adventure for such a veins \n",
            "cannot be a messenger of romeo \n",
            "thou from thy heart \n",
            "that would have kill'd \n",
            "such be one \n",
            "within the infant rind of this other maid \n",
            "them weep ye now seeing she is \n",
            "twenty of thy valour of charge \n",
            "night \n",
            "sir i would adventure for such a friend \n",
            "\n",
            "that i were to see that you know near thee \n",
            "lay rich capulet and gregory of friar at true \n",
            "why she hath praised him girl straight \n",
            "sir i would not keep from the very sound \n",
            "thou need'st not for the sudden day \n",
            "by my soul i'll come the county i will \n",
            "man for cracking nuts having no note of this \n",
            "in his needy shop lies \n",
            "' o musicians thou not my rapier in company \n",
            "gentleman the bitter sweeting it is broad capulet \n",
            "and down their soul good rest \n",
            "\n",
            "gentleman the lark itself lips o god in heaven \n",
            "and portentous must this humour prove \n",
            "only son of these poor so \n",
            "thou changed i were prevails not \n",
            "for the wanton summer to me \n",
            "sir i would adventure for such a night \n",
            "' o o my extremes and the maids cries of \n",
            "romeo of friar laurence \n",
            "him come to thy montague \n",
            "by his wisdom hastes our marriage \n",
            "within from this hour she promised to \n",
            "sir i would adventure for such a veins \n",
            "heart for sending me about his "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fx4Z6NmycUA",
        "colab_type": "text"
      },
      "source": [
        "## Save Poetry in a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mVmfFmkycUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    with open(save_file_path, 'w') as f:\n",
        "        f.write(corpus)\n",
        "        f.close()\n",
        "except:\n",
        "    print(\"\\nWrite was Unsuccessful\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h5L9GAaycUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHJH5zlAycUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}